# -*- coding: utf-8 -*-
"""Progetto - Cyber Security per la sanità tramite Reinforcement Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GTBWv4FbJBM774Inape4clWBb32O_auC

#Introduzione

DeepGuard Inc., un'azienda leader nel settore della sicurezza informatica per le industrie sanitarie, si trova a fronteggiare un aumento della complessità e sofisticazione degli attacchi informatici. La necessità di proteggere le informazioni sensibili dei pazienti e garantire la conformità normativa è cruciale per mantenere la fiducia dei clienti e la sicurezza dei dati.

**Benefici del Progetto**: L'implementazione di algoritmi avanzati di Reinforcement Learning per simulare e mitigare scenari di attacco e difesa offre vantaggi significativi:

- **Miglioramento della Difesa**: Addestramento di agenti difensivi in scenari simulati per sviluppare strategie di protezione robuste e adattabili.
- **Identificazione delle Vulnerabilità**: Simulazione di attacchi per identificare e risolvere vulnerabilità nei sistemi di rete prima che possano essere sfruttate da attaccanti reali.
- **Innovazione Tecnologica**: Utilizzo di tecniche avanzate di Reinforcement Learning per promuovere l'innovazione e migliorare continuamente le capacità di difesa informatica.
- **Ottimizzazione delle Risorse**: Automatizzazione della simulazione di attacchi e difese per ridurre il carico di lavoro umano e ottimizzare l'allocazione delle risorse aziendali.

**Dettagli del Progetto**: GreenGuard Solutions ha incaricato lo sviluppo di una soluzione avanzata per la sicurezza informatica basata su algoritmi di Reinforcement Learning. Il progetto si focalizza sull'applicazione di due principali algoritmi all'interno dell'ambiente gym-idsgame, specializzato in simulazioni di attacco e difesa in reti informatiche.

**Obiettivi del Progetto**:

 1. Algoritmo SARSA: Utilizzare SARSA per affrontare scenari di "random attack" nell'ambiente gym-idsgame.
 2. Algoritmo DDQN: Implementare Double Deep Q-Network (DDQN) con PyTorch per risolvere scenari di "random attack" e "maximal attack".

**Deliverable**: Il progetto richiede la consegna di un notebook Google Colab suddiviso in due sezioni principali, ciascuna dedicata a uno dei due algoritmi di Reinforcement Learning. Ogni sezione dovrà contenere spiegazioni dettagliate delle soluzioni proposte, motivando le scelte effettuate e analizzando i risultati ottenuti.

**Motivazione del Progetto**: GreenGuard Solutions pone la massima priorità sulla sicurezza informatica nel settore sanitario. L'utilizzo di algoritmi avanzati di Reinforcement Learning per simulare e mitigare rischi cibernetici consente all'azienda di rafforzare le sue capacità difensive e proteggere le reti di computer contro attacchi sempre più sofisticati. Automatizzando la simulazione di attacchi, GreenGuard ottimizza l'efficienza operativa e si conferma come leader nell'innovazione tecnologica per la sicurezza informatica nel settore sanitario.

Link all’ambiente di simulazione: https://github.com/Limmen/gym-idsgame

Con questo progetto, GreenGuard Solutions mira a garantire la sicurezza dei dati sensibili dei pazienti e a mantenere l'integrità delle informazioni nel contesto sanitario, proteggendo le reti di computer da minacce informatiche sempre più avanzate e persistenti.

#Progetto

##Preparazione dell'Ambiente
"""

!git clone https://github.com/Limmen/gym-idsgame.git

!pip install -e gym-idsgame

!pip show gym-idsgame

!pytest

# Commented out IPython magic to ensure Python compatibility.
import os
os.getcwd()
# %cd /content/gym-idsgame
import gymnasium as gym
from gym_idsgame.envs.idsgame_env import IdsGameEnv
import numpy as np
import random
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque

# Creazione dell'ambiente
env_name = "idsgame-maximal_attack-v21"
env = gym.make(env_name)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""##Analisi e Definizione dell'Ambiente"""

# Salviamo i metodi originali
original_reset = IdsGameEnv.reset
original_step = IdsGameEnv.step

def patched_reset(self, seed=None, options=None, **kwargs):
    # Ignoriamo 'options' e passiamo seed e gli altri kwargs al reset originale
    obs, info = original_reset(self, seed=seed, **kwargs)
    # Assicuriamoci che l'osservazione sia nel range [0, 9] e del tipo int32
    obs = np.clip(obs, 0, 9).astype(np.int32)
    return obs, info

def patched_step(self, action):
    obs, reward, terminated, truncated, info = original_step(self, action)
    # Correggiamo l'osservazione
    obs = np.clip(obs, 0, 9).astype(np.int32)
    # Se il reward è una tupla, prendiamo il primo elemento
    if isinstance(reward, (tuple, list, np.ndarray)):
        reward = reward[0]
    return obs, reward, terminated, truncated, info

# Applichiamo le patch
IdsGameEnv.reset = patched_reset
IdsGameEnv.step = patched_step

# Funzione per processare l'azione
def process_action(action):
    if not isinstance(action, (tuple, list)):
        if hasattr(env.action_space, 'spaces'):
            return tuple(space.sample() for space in env.action_space.spaces)
        return (action, action)
    return action

# Funzione per stampare informazioni sull'ambiente
def print_env_info(env, state, action, next_state, reward, terminated, truncated, info):
    print(f"\n>>> ID Ambiente: {env.spec.id}")
    print(f"Spazio Stati: {env.observation_space}")
    print(f"Spazio Azioni: {env.action_space}")
    print(f"\nStato Iniziale:\n{state}")
    print(f"Azione Eseguita: {action}")
    print(f"\nNuovo Stato:\n{next_state}")
    print(f"Reward: {reward}, Terminato: {terminated}, Troncato: {truncated}")
    print(f"Info Extra: {info}\n")

# Funzione per visualizzare lo stato come heatmap
def plot_state(state, title="Stato dell'Ambiente"):
    plt.figure(figsize=(6, 3))
    sns.heatmap(state, annot=True, cmap="coolwarm", cbar=False, linewidths=0.5)
    plt.title(title)
    plt.show()

# Stampa informazioni base dell'ambiente
print("ID ambiente:", env.spec.id)
print("Spazio degli stati:", env.observation_space)
print("Spazio delle azioni:", env.action_space)

# Reset dell'ambiente e visualizzazione dello stato iniziale
state, info = env.reset()
print("\nStato iniziale:")
print(state)
print("Informazioni iniziali:", info)
plot_state(state, "Stato Iniziale")

# Campionamento azione e trasformazione
action = env.action_space.sample()
action = process_action(action)

# Esecuzione di un'azione nell'ambiente
next_state, reward, terminated, truncated, info = env.step(action)
plot_state(next_state, "Stato Dopo l'Azione")

# Stampa dettagliata dell'interazione
print_env_info(env, state, action, next_state, reward, terminated, truncated, info)

# Verifica se lo spazio degli stati è discreto o continuo
if hasattr(env.observation_space, 'n'):
    print("\nLo spazio degli stati è discreto con", env.observation_space.n, "stati.")
elif hasattr(env.observation_space, 'shape'):
    print("\nLo spazio degli stati è continuo, forma:", env.observation_space.shape)

"""##Sviluppo dell'Algoritmo SARSA

###Funzioni
"""

def discretize_state(state):
    if isinstance(state, np.ndarray):
        return tuple(int(x) for x in state.flatten())
    elif isinstance(state, (int, float)):
        return (int(state),)
    elif isinstance(state, (list, tuple)):
        return tuple(int(x) for x in flatten(state))
    else:
        raise TypeError("Tipo di state non supportato: {}".format(type(state)))

def flatten(x):
    """
    Appiattisce ricorsivamente una struttura iterabile.
    Se x è un array NumPy, lo converte in una lista piatta.
    """
    if isinstance(x, np.ndarray):
        return x.flatten().tolist()
    elif isinstance(x, (list, tuple)):
        for item in x:
            yield from flatten(item)
    else:
        yield x

# Funzione per convertire azione
def convert_action(action):
    if isinstance(action, (tuple, list)):
        return action
    else:
        return (action, action)

# Funzione epsilon-greedy
def choose_action(state, Q, epsilon, action_space):
    if random.random() < epsilon:
        return action_space.sample()
    else:
        state_key = discretize_state(state)
        if state_key not in Q:
            Q[state_key] = np.zeros(action_space.n)
        return int(np.argmax(Q[state_key]))

"""###Addestramento"""

# Parametri SARSA
n_episodes    = 1000
alpha         = 0.1
gamma         = 0.99
epsilon       = 1.0
min_epsilon   = 0.01
epsilon_decay = 0.995

# Q-table
# Dizionario Q-learning
Q = {}

# Liste analisi
rewards_per_episode_sarsa = []
rewards_per_step_per_episode = []
steps_per_episode = []
states_per_episode = []
actions_per_episode_sarsa = []
action_counts = {i: 0 for i in range(env.action_space.n)}
epsilon_history_sarsa = []

for episode in range(n_episodes):
    state, _ = env.reset()
    state_key = discretize_state(state)

    if state_key not in Q:
        Q[state_key] = np.zeros(env.action_space.n)

    action = choose_action(state, Q, epsilon, env.action_space)

    done = False
    episode_reward = 0
    episode_steps = 0
    visited_states = set()
    episode_actions = []
    rewards_per_step = []

    while not done:
        s_action = convert_action(action)
        next_state, reward, terminated, truncated, info = env.step(s_action)
        done = terminated or truncated

        next_state_key = discretize_state(next_state)
        if next_state_key not in Q:
            Q[next_state_key] = np.zeros(env.action_space.n)

        next_action = choose_action(next_state, Q, epsilon, env.action_space)
        reward_scalar = reward[0] if isinstance(reward, (tuple, list, np.ndarray)) else reward

        # Aggiornamento SARSA
        Q[state_key][action] = Q[state_key][action] + alpha * (
            reward_scalar + gamma * Q[next_state_key][next_action] - Q[state_key][action]
        )

        # Aggiornamento metriche
        episode_reward += reward_scalar
        episode_steps += 1
        visited_states.add(state_key)
        action_counts[action] += 1
        episode_actions.append(action)
        rewards_per_step.append(reward_scalar)

        state = next_state
        state_key = next_state_key
        action = next_action

    # Salvataggio dati raccolti
    rewards_per_episode_sarsa.append(episode_reward)
    rewards_per_step_per_episode.append(rewards_per_step)
    steps_per_episode.append(episode_steps)
    states_per_episode.append(list(visited_states))
    actions_per_episode_sarsa.append(episode_actions)
    epsilon_history_sarsa.append(epsilon)

    # Aggiorniamo epsilon
    epsilon = max(min_epsilon, epsilon * epsilon_decay)

    if (episode + 1) % 100 == 0:
        print(f"Episode {episode + 1}/{n_episodes} completato.")

print("Addestramento SARSA completato!")

"""###Analisi e Valutazioni

####Analisi del Reward
"""

#Cumulative Reward
cumulative_rewards = np.cumsum(rewards_per_episode_sarsa)
plt.figure(figsize=(12, 5))
plt.plot(cumulative_rewards, label="Cumulative Reward", color='purple')
plt.xlabel("Episodi")
plt.ylabel("Cumulative Reward")
plt.title("Cumulative Reward over Episodes")
plt.legend()
plt.grid(True)
plt.show()

"""Il grafico della Cumulative Reward mostra l'andamento dei reward accumulati nel corso degli episodi durante l'addestramento dell'agente SARSA. Dall'output testuale possiamo osservare che:

Nei primi episodi il reward cumulativo inizia con valori modesti, il che indica che l'agente, nella fase iniziale, sta accumulando piccoli reward mentre esplora l'ambiente.

Con il passare degli episodi, il valore cumulativo tende a crescere in maniera progressiva.

Nonostante alcune fluttuazioni (alcuni episodi in cui il valore cumulativo potrebbe stagnare o addirittura diminuire leggermente), il trend generale è di crescita, il che suggerisce che l'agente sta migliorando la sua performance nel tempo.

La crescita costante della reward cumulativa evidenzia che il processo di apprendimento sta funzionando, anche se la progressione non è completamente monotona, tipico in ambienti complessi in cui l'addestramento può incontrare momenti di esplorazione e sperimentazione che influenzano temporaneamente i risultati.

In sintesi, il grafico indica una tendenza positiva e un apprendimento progressivo, con l'agente che accumula reward crescenti durante l'addestramento. Questo è un segnale incoraggiante che l'approccio SARSA sta portando a miglioramenti nelle performance dell'agente nel tempo.
"""

# Reward per Episodio con Moving Average
window_size = 50
plt.figure(figsize=(12, 5))
plt.plot(rewards_per_episode_sarsa, label="Reward per Episodio", alpha=0.5)
if len(rewards_per_episode_sarsa) >= window_size:
    moving_avg = np.convolve(rewards_per_episode_sarsa, np.ones(window_size)/window_size, mode='valid')
    plt.plot(range(window_size - 1, len(rewards_per_episode_sarsa)), moving_avg, label=f"Moving Average (window={window_size})", color='red')
plt.xlabel("Episodi")
plt.ylabel("Reward Totale")
plt.title("Reward per Episodio con Moving Average")
plt.legend()
plt.grid(True)
plt.show()

"""Il grafico mostra il valore della media mobile del reward per episodio, calcolata su una finestra di 50 episodi, e serve a evidenziare la tendenza generale delle performance, attenuando le fluttuazioni episodio per episodio.

Osservazioni principali:

- **Valori Iniziali**:
I primi valori della moving average partono intorno a 1.28. Questo indica che, nelle fasi iniziali dell'addestramento, l'agente ottiene reward medi relativamente modesti.

- **Diminuzione Iniziale**:
La media mobile scende progressivamente fino a raggiungere valori intorno a 0.5, suggerendo che in alcuni episodi l'agente ha avuto performance più basse, un comportamento tipico nella fase esplorativa iniziale.

- **Successivo Aumento e Stabilizzazione**:
Dopo questo periodo di calo, la media mobile inizia ad aumentare gradualmente, passando per valori compresi tra 1.0 e 1.2, e poi continua a salire fino a raggiungere valori superiori, come 1.7–1.8 o oltre nei successivi episodi. Questo trend crescente evidenzia che l'agente sta migliorando le proprie performance con l'addestramento, probabilmente imparando una politica più efficace nel rispondere alle dinamiche dell'ambiente.

- **Oscillazioni Locali**:
Alcune oscillazioni o fluttuazioni locali nel grafico potrebbero riflettere fasi in cui l'agente esplora nuove strategie o si adatta a particolari transizioni ambientali. Queste variazioni sono comuni in ambienti complessi e indicano un bilanciamento tra esplorazione e sfruttamento.

In conclusione, il grafico della media mobile del reward per episodio evidenzia chiaramente un trend di miglioramento: nonostante una fase iniziale con reward medi più bassi, l'agente mostra una progressiva crescita nelle performance, con valori che aumentano e tendono a stabilizzarsi nei livelli più alti. Questo comportamento è indicativo di un apprendimento efficace, sebbene le oscillazioni suggeriscano che l'ambiente presenta dinamiche variabili che spingono l'agente a continuare a esplorare e affinare la propria politica.
"""

# Rolling Variance dei Reward
if len(rewards_per_episode_sarsa) >= window_size:
    rolling_variance = [np.var(rewards_per_episode_sarsa[i:i+window_size])
                        for i in range(len(rewards_per_episode_sarsa) - window_size + 1)]
    plt.figure(figsize=(12, 5))
    plt.plot(rolling_variance, label=f"Rolling Variance (window={window_size})", color='orange')
    plt.xlabel("Episodi (indice finestra)")
    plt.ylabel("Variance of Reward")
    plt.title("Rolling Variance of Reward")
    plt.legend()
    plt.grid(True)
    plt.show()

"""Il grafico della **rolling variance** mostra l'andamento della varianza mobili dei reward per episodio, calcolata su una finestra di 50 episodi, per analizzare la variabilità delle performance dell'agente nel tempo.

**Osservazioni principali:**
- **Iniziale Alta Varianza**

  Nei primi episodi, la varianza si colloca già intorno a valori superiori a 2.5, evidenziando un’elevata fluttuazione nei reward. Ciò è tipico della fase iniziale, in cui l’agente esplora in modo aggressivo e non ha ancora una politica stabilizzata.

- **Stabilizzazione e Fluttuazioni Intermedie**

  Con il progredire dell’addestramento, la varianza si stabilizza su un range compreso approssimativamente tra 2.0 e 3.4, mostrando comunque picchi periodici che raggiungono o superano 3.5. Questi valori riflettono la presenza di episodi con reward molto diversi tra loro, segno che l’agente sta ancora affinando la propria strategia.

- **Picchi Periodici**

  Nel corso dell’apprendimento, si osservano più volte valori superiori a 3.5. Tali picchi indicano momenti in cui le performance dell’agente subiscono variazioni ampie, verosimilmente a causa di esplorazioni più spinte o di situazioni in cui la strategia non è ancora del tutto ottimizzata.

- **Tendenza Finale**

  Nella parte conclusiva, la varianza tende a diminuire rispetto ai picchi precedenti e si colloca attorno a valori compresi tra 2.0 e 2.5, senza però scendere fino a 1.5. Ciò suggerisce che l’agente ha acquisito maggiore coerenza nelle proprie decisioni, pur conservando un certo livello di variabilità legato all’esplorazione o alla complessità dell’ambiente.

Il grafico evidenzia un’evoluzione tipica di un agente in fase di apprendimento: si parte da una notevole variabilità iniziale, si passa a una fase con fluttuazioni anche elevate, e infine si raggiunge una relativa stabilizzazione con valori di varianza più contenuti. Rimangono tuttavia picchi occasionali, indicativi del fatto che l’agente, pur avendo migliorato la propria strategia, incontra ancora situazioni in cui le performance variano in modo significativo.
"""

#Istogramma Distribuzione Reward
plt.figure(figsize=(8, 5))
plt.hist(rewards_per_episode_sarsa, bins=30, color='skyblue', edgecolor='black')
plt.xlabel("Reward Totale")
plt.ylabel("Frequenza")
plt.title("Distribuzione del Reward Totale per Episodio")
plt.grid(True)
plt.show()

"""Il grafico mostra la distribuzione dei reward totali per episodio suddivisa in intervalli.

I dati indicano che durante l'addestramento c'è una notevole variabilità nei reward: mentre alcuni episodi terminano con performance molto basse (reward negativi o vicino a zero), un numero consistente di episodi raggiunge reward intorno a 3.

Questo suggerisce che, nonostante la fase iniziale di esplorazione possa portare a performance scarse, l'agente è in grado di apprendere strategie che producono reward più elevati nel tempo.

La presenza di numerose occorrenze nei range negativi potrebbe indicare la difficoltà iniziale o la presenza di momenti di esplorazione aggressiva, ma l'accumulo di episodi con reward elevati dimostra una progressiva convergenza verso politiche più efficaci.

In sintesi, il grafico evidenzia un miglioramento generale nelle performance, con una distribuzione che si sposta progressivamente verso reward più alti, nonostante la presenza di episodi con risultati inferiori.
"""

# Statistiche del Reward
mean_reward = np.mean(rewards_per_episode_sarsa)
median_reward = np.median(rewards_per_episode_sarsa)
std_reward = np.std(rewards_per_episode_sarsa)
max_reward = np.max(rewards_per_episode_sarsa)
min_reward = np.min(rewards_per_episode_sarsa)
print("Statistiche del Reward Totale per Episodio:")
print(f"Media: {mean_reward:.2f}")
print(f"Mediana: {median_reward:.2f}")
print(f"Deviazione Standard: {std_reward:.2f}")
print(f"Massimo: {max_reward:.2f}")
print(f"Minimo: {min_reward:.2f}")
best_reward = max(rewards_per_episode_sarsa)
best_episode = rewards_per_episode_sarsa.index(best_reward) + 1
print(f"Miglior reward ottenuto: {best_reward:.2f} all'episodio {best_episode}")

"""- **Media**: La performance media è positiva, indicando che la politica appresa tende a generare risultati vantaggiosi. Tuttavia, per confermare se è inferiore a DDQN, serve un confronto diretto con la sua media.
- **Mediana**: Il fatto che la mediana sia 1 suggerisce che almeno il 50% degli episodi ottiene un reward pari o superiore a 1, segnalando una buona consistenza nei risultati.
- **Deviazione Standard**: La variabilità intorno al valore medio è piuttosto elevata (1.79), indicando una notevole oscillazione nei reward ottenuti, probabilmente dovuta alla variabilità intrinseca dell'ambiente.
- **Massimo & Minimo**: I valori estremi indicano che, sebbene in alcuni episodi il reward raggiunga il massimo teorico (3.00), in altri episodi la politica può incorrere in decisioni sbagliate che portano a un reward negativo (-2.00).
- **Miglior reward ottenuto**: L'episodio 2 rappresenta un caso di eccellenza, dimostrando che la politica può effettivamente raggiungere il risultato ottimale.

SARSA mostra una performance generalmente positiva, ma con una variabilità significativa nei risultati. Per un confronto completo con DDQN, sarebbe utile esaminare anche la distribuzione dei reward e la stabilità dell'apprendimento.

####Analisi delle Azioni
"""

plt.figure(figsize=(12, 5))
plt.bar(list(action_counts.keys()), list(action_counts.values()), color='skyblue', edgecolor='black')
plt.xlabel("Azione")
plt.ylabel("Frequenza")
plt.title("Action Frequency Distribution during Training")
plt.grid(True)
plt.show()

"""Il grafico mostra la frequenza con cui ciascuna azione viene scelta durante l'addestramento. Dall'output testuale emergono alcune osservazioni interessanti:

- Azioni Frequenti:
 - Azione 14 risulta la più selezionata, indicando che questa azione viene ritenuta molto utile dall'agente.
 - Anche Azioni 0, 1, 9 e 15 mostrano frequenze elevate, suggerendo che anch'esse giocano un ruolo chiave nella strategia adottata.

- Azioni Poco Utilizzate:
 - Diverse azioni, quali 3, 4, 10,11, 12, 13, 16 e 19, hanno frequenze molto basse (tutte inferiori a 100 occorrenze). Questo indica che l'agente le seleziona raramente, probabilmente perché non contribuiscono significativamente all'aumento del reward.

- Interpretazione Strategica:
 - La distribuzione asimmetrica delle azioni suggerisce che l'agente ha identificato alcune azioni come maggiormente vantaggiose per massimizzare il reward, mentre altre sono utilizzate solo in circostanze specifiche o per esplorare l'ambiente.
 - Un'azione fortemente preferita (come l'azione 1) può indicare che, in molte situazioni, essa porta a transizioni di stato favorevoli. Al contrario, le azioni con frequenza bassa potrebbero essere considerate meno efficaci o utilizzate solo in situazioni particolari.

In sintesi, il grafico evidenzia un pattern di selezione delle azioni in cui l'agente si concentra su alcune azioni chiave, mentre altre vengono utilizzate sporadicamente, riflettendo la strategia appresa durante l'addestramento.

####Analisi della Lunghezza degli Episodi
"""

plt.figure(figsize=(12, 5))
plt.plot(steps_per_episode, label="Steps per Episode", color='brown')
plt.xlabel("Episodi")
plt.ylabel("Numero di Steps")
plt.title("Numero di Steps per Episodio")
plt.legend()
plt.grid(True)
plt.show()

# Calcolo delle statistiche di base
tot_episodi = len(steps_per_episode)
mean_steps = np.mean(steps_per_episode)
min_steps = np.min(steps_per_episode)
max_steps = np.max(steps_per_episode)
correlation = np.corrcoef(steps_per_episode, rewards_per_episode_sarsa)[0, 1]

print("\nStatistiche Generali:")
print(f"Numero totale di episodi: {tot_episodi}")
print(f"Media Steps per Episode: {mean_steps:.2f}")
print(f"Minimo Steps per Episode: {min_steps}")
print(f"Massimo Steps per Episode: {max_steps}")
print(f"Correlazione tra Steps e Reward: {correlation:.2f}")

"""- **Tendenza Centrale**: La media delle ricompense per episodio è intorno a 0.29, con una mediana di 0.00. Questo suggerisce che, sebbene ci siano alcuni episodi con ricompense positive, la maggior parte degli episodi porta a ricompense basse o negative, indicando che l'agente sta ancora apprendendo come ottimizzare le sue azioni.

- **Variabilità**: La deviazione standard delle ricompense è abbastanza elevata, pari a 1.12, segnalando una notevole variabilità nei risultati. La distribuzione delle ricompense non è concentrata attorno a valori centrali, ma piuttosto mostra una distribuzione che varia molto, il che implica che l'agente sta esplorando una varietà di strategie con esiti molto diversi.

- **Punti Estremi**: L'episodio con la ricompensa massima di 3.0 indica che, in alcuni casi, l'agente riesce a ottenere ottimi risultati. Tuttavia, ci sono anche episodi con ricompense negative come -1.0 che suggeriscono che l'agente sta affrontando difficoltà in certi scenari, compiendo scelte subottimali o cadendo in penalità.

**Interpretazione**

La distribuzione delle ricompense mostra una certa fluttuazione, con episodi che variano notevolmente tra ricompense negative e positive. Ciò suggerisce che, sebbene l'agente stia imparando a prendere decisioni efficaci in alcuni casi, non ha ancora acquisito una strategia stabile per tutti gli scenari. La presenza di episodi con ricompense positive potrebbe indicare che l'agente ha iniziato a identificare azioni ottimali in alcune situazioni, ma il margine di miglioramento è ancora significativo, specialmente nei casi in cui le ricompense sono negative. Ulteriori analisi potrebbero aiutare a migliorare la stabilità del comportamento dell'agente e a ridurre la variabilità nei risultati.
"""

plt.figure(figsize=(12, 5))
plt.scatter(steps_per_episode, rewards_per_episode_sarsa, alpha=0.7, color='green')
plt.xlabel("Steps per Episode")
plt.ylabel("Reward per Episode")
plt.title("Reward vs. Episode Length")
plt.grid(True)
plt.show()

# Analisi della correlazione tra reward per episodio e lunghezza episodio
correlation = np.corrcoef(steps_per_episode, rewards_per_episode_sarsa)[0,1]
print("\n=== Correlazione tra Steps per Episodio e Reward ===")
print(f"Correlazione Pearson: {correlation:.4f}")

# Stampiamo alcuni episodi rappresentativi
best_episode_idx = np.argmax(steps_per_episode)
worst_episode_idx = np.argmin(steps_per_episode)

print("\n=== Esempi di Episodi ===")
print(f"Miglior Episodio (indice {best_episode_idx}): {steps_per_episode[best_episode_idx]} steps, Reward = {rewards_per_episode_sarsa[best_episode_idx]:.2f}")
print(f"Peggior Episodio (indice {worst_episode_idx}): {steps_per_episode[worst_episode_idx]} steps, Reward = {rewards_per_episode_sarsa[worst_episode_idx]:.2f}")

"""**Forza della Correlazione**

Il coefficiente di correlazione di Pearson è 0.2156, indicando una correlazione positiva ma debole tra la lunghezza dell'episodio (numero di passi) e la ricompensa. Questo suggerisce che, sebbene alcuni episodi più lunghi possano portare a ricompense più alte, la relazione tra i due fattori non è particolarmente forte o lineare.

**Distribuzione dei Risultati**:

- **Miglior Episodio**(Indice 712): Con 16 passi, l'agente ha ottenuto un reward di 2.00, dimostrando un comportamento relativamente efficace, sebbene la durata non sia estremamente lunga.
- **Peggior Episodio** (Indice 0): In solo 1 passo, l'agente ha accumulato un reward negativo di -1.00, suggerendo che episodi brevi possano portare a azioni subottimali o a penalità dall'ambiente.

**Interpretazione**

Nel grafico scatterplot, si nota che la maggior parte degli episodi brevi tende a presentare ricompense negative, mentre gli episodi più lunghi sono distribuiti tra ricompense positive e negative. Questo pattern suggerisce che la durata dell'episodio non è un indicatore chiaro della qualità della strategia dell'agente. Sebbene episodi più lunghi possano in alcuni casi portare a ricompense positive, la correlazione debole indica che l'agente potrebbe comunque compiere azioni subottimali anche in questi episodi.

In conclusione, l'analisi visiva suggerisce che l'agente potrebbe beneficiare di strategie di ottimizzazione che trovino un equilibrio tra la lunghezza dell'episodio e l'efficacia delle azioni, migliorando la stabilità delle ricompense. Potrebbe essere utile esplorare tecniche di apprendimento come il fine-tuning della politica di esplorazione o l'uso di metodi di ottimizzazione dei passi per ridurre le fluttuazioni nelle ricompense.

####Evoluzione di Epsilon
"""

plt.figure(figsize=(12, 5))
plt.plot(epsilon_history_sarsa, label="Valore di Epsilon", color='green')
plt.xlabel("Episodi")
plt.ylabel("Epsilon")
plt.title("Evoluzione di Epsilon durante l'Addestramento SARSA")
plt.legend()
plt.grid(True)
plt.show()

"""Il grafico, insieme all'output testuale, mostra in maniera dettagliata come il parametro epsilon, che regola l'esplorazione, decresca lungo tutto il percorso di addestramento (da 1.0 all'episodio 1 fino a circa 0.0100 all'episodio 1000). Di seguito alcuni punti chiave:

1. **Fase Iniziale** (Episodi 1–500):

- L'addestramento inizia con un epsilon elevato (1.0000), il che significa che nelle prime fasi l'agente esplora l'ambiente in maniera completamente casuale.
- Durante i primi 500 episodi, epsilon decresce rapidamente, passando da 1.0000 a circa 0.0820. Questo rapido decremento favorisce una prima fase intensa di esplorazione, durante la quale l'agente raccoglie una vasta gamma di esperienze.

2. **Fase Avanzata** (Episodi 501–1000):

- A partire dall'episodio 501, epsilon continua a ridursi in maniera più lenta e graduale, scendendo da circa 0.0816 fino a raggiungere il valore minimo stabilito di 0.0100 all'episodio 1000.
- Questo lento decremento indica che l'agente sta progressivamente riducendo l'esplorazione e concentrandosi sull'exploitation della politica appresa.
- L'approccio finale, con epsilon intorno a 0.0100, significa che l'agente esegue quasi esclusivamente azioni basate sulla conoscenza acquisita, limitando drasticamente le scelte casuali.

3. **Implicazioni per l'Apprendimento**:

- La transizione da un'alta esplorazione a un quasi completo sfruttamento della politica è fondamentale per la stabilizzazione del comportamento dell'agente.
- Il decadimento regolare e controllato di epsilon assicura che, inizialmente, l'agente possa esplorare diverse strategie e, man mano, affinare la propria politica basandosi sulle esperienze accumulate.
- Il fatto che epsilon raggiunga e mantenga un valore basso (0.0100) nelle ultime fasi indica che il processo di apprendimento si è stabilizzato, permettendo all'agente di operare con una politica quasi deterministica.

In sintesi, il grafico mostra un'evoluzione ben orchestrata di epsilon, che passa da una fase di massima esplorazione ad una fase in cui l'agente sfrutta la propria esperienza, un pattern tipico e desiderabile per ottenere un apprendimento stabile e performante nel tempo.

####Analisi del TD Error
"""

TD_errors = []
visited_states = set()
Q_values_mean = []
Q_values_std = []
states_visited_per_episode = []

# Calcolo della media mobile per il TD Error
window_size_td_sarsa = 10
if len(TD_errors) >= window_size_td_sarsa:
    smoothed_td_errors_sarsa = np.convolve(TD_errors, np.ones(window_size_td_sarsa)/window_size_td_sarsa, mode='valid')
else:
    smoothed_td_errors_sarsa = TD_errors

for episode in range(n_episodes):
    state, _ = env.reset()
    state_key = discretize_state(state)
    if state_key not in Q:
        Q[state_key] = np.zeros(env.action_space.n)

    action = choose_action(state, Q, epsilon, env.action_space)
    done = False
    episode_TD_errors = []

    while not done:
        s_action = convert_action(action)
        next_state, reward, terminated, truncated, info = env.step(s_action)
        done = terminated or truncated

        next_state_key = discretize_state(next_state)
        if next_state_key not in Q:
            Q[next_state_key] = np.zeros(env.action_space.n)

        next_action = choose_action(next_state, Q, epsilon, env.action_space)
        reward_scalar = reward[0] if isinstance(reward, (tuple, list, np.ndarray)) else reward

        # TD Error Calculation
        td_error = reward_scalar + gamma * Q[next_state_key][next_action] - Q[state_key][action]
        Q[state_key][action] += alpha * td_error
        episode_TD_errors.append(abs(td_error))

        visited_states.add(state_key)
        state = next_state
        state_key = next_state_key
        action = next_action

    TD_errors.append(np.mean(episode_TD_errors))
    states_visited_per_episode.append(len(visited_states))
    Q_values_mean.append(np.mean([np.mean(Q[s]) for s in Q]))
    Q_values_std.append(np.std([np.std(Q[s]) for s in Q]))

    epsilon = max(min_epsilon, epsilon * epsilon_decay)

# Statistiche generali sul TD Error
td_mean = np.mean(TD_errors)
td_min = np.min(TD_errors)
td_max = np.max(TD_errors)
td_std = np.std(TD_errors)

print(f"TD Error Medio: {td_mean:.4f}")
print(f"TD Error Minimo: {td_min:.4f}")
print(f"TD Error Massimo: {td_max:.4f}")
print(f"TD Error Deviazione Standard: {td_std:.4f}")

# Analisi dell'andamento nel tempo
if len(TD_errors) > 10:
    td_last_10_mean = np.mean(TD_errors[-10:])
    print(f"TD Error Medio negli ultimi 10 episodi: {td_last_10_mean:.4f}")

"""Questi valori indicano che, complessivamente, la maggior parte delle transizioni presenta errori relativamente contenuti, sebbene vi siano alcuni outlier (fino a 2.0477) che aumentano la variabilità complessiva.

**Fase Finale di Addestramento**

  Il fatto che il TD error medio negli ultimi 10 episodi sia inferiore rispetto alla media complessiva (0.1799 vs. 0.2067) suggerisce un miglioramento nella capacità del modello di stimare i valori correttamente nelle fasi finali. In altre parole, l'agente sembra aver appreso a ridurre l'errore nelle sue previsioni, evidenziando una convergenza più stabile.

**Considerazioni Finali**

- La presenza di alcuni picchi elevati indica che, sebbene la maggior parte dei TD error sia bassa, in alcune transizioni il modello può incontrare situazioni particolarmente complesse o non ancora ben apprese.
- L'abbassamento del TD error nelle ultime fasi di addestramento è un segnale positivo, poiché dimostra che il modello sta migliorando nel tempo.
- Un'analisi approfondita di quegli outlier potrebbe essere utile per identificare e gestire eventuali situazioni problematiche o per ottimizzare ulteriormente il processo di apprendimento.

In sintesi, i risultati evidenziano un buon apprendimento generale con un miglioramento nelle fasi finali, nonostante la presenza di alcune transizioni con errori più elevati.
"""

# TD Error Plot
plt.figure(figsize=(12, 5))
plt.plot(TD_errors, label='Mean TD Error', color='blue')
plt.xlabel('Episodi')
plt.ylabel('TD Error')
plt.title("Andamento del TD Error durante l'Addestramento")
plt.legend()
plt.grid(True)
plt.show()

"""L'analisi del TD Error lungo l'intero processo di addestramento (episodi 1–1000) mostra una notevole variabilità nei valori, che fornisce importanti indicazioni sul processo di aggiornamento delle Q-values:

- **Valori Ripetuti e Oscillazioni**:

  In molti episodi si osservano TD Error pari a valori “standard” come 3.0000, 2.0000 e -1.0000. Questi valori ricorrenti indicano che, in numerose situazioni, la differenza tra il valore stimato e il target è sostanziale, portando a significativi aggiornamenti delle Q-values.

- **Variazioni Moderati**:

  Alcuni episodi mostrano invece valori leggermente diversi, come 2.9275, 3.1156, -0.9268 e -1.1168. Questi valori moderati suggeriscono che, in certi momenti, le discrepanze sono meno marcate, probabilmente dovute a oscillazioni più sottili nel processo di apprendimento.

- **Andamento Complessivo**:

  L'analisi integrata evidenzia che, sia nella prima metà (episodi 1–500) sia nella seconda metà (episodi 501–1000) dell'addestramento, il TD Error presenta una forte componente di instabilità. Pur essendo attesi aggiornamenti significativi nelle fasi iniziali, la persistenza di valori elevati in molti episodi suggerisce che l'agente continua a confrontarsi con discrepanze rilevanti tra la stima corrente e il target.

- **Implicazioni per l'Apprendimento**:

  Questo andamento indica che, nonostante l'agente stia apprendendo e migliorando la sua politica, il processo di aggiornamento delle Q-values è ancora soggetto a forti fluttuazioni. Tali oscillazioni possono essere legate alla complessità dell'ambiente o a fasi di esplorazione intensificata che interrompono temporaneamente la convergenza della politica.

In sintesi, il TD Error, monitorato per ciascun episodio, rimane elevato e variabile, segno che l'agente effettua aggiornamenti consistenti ma non completamente stabili. Queste osservazioni suggeriscono che, sebbene il processo di apprendimento stia progredendo, ci sono margini per ulteriori ottimizzazioni, ad esempio nell'adeguamento del tasso di apprendimento o nella gestione dell'esplorazione, al fine di ridurre le oscillazioni e promuovere una convergenza più uniforme della politica.
"""

# Moving Average of TD Error
window_size = 50
if len(TD_errors) >= window_size:
    moving_avg_td = np.convolve(TD_errors, np.ones(window_size)/window_size, mode='valid')
    plt.figure(figsize=(12, 5))
    plt.plot(moving_avg_td, label=f"Moving Avg TD Error (window={window_size})", color='blue')
    plt.xlabel("Episodi")
    plt.ylabel("TD Error")
    plt.title("Andamento del TD Error")
    plt.legend()
    plt.grid(True)
    plt.show()

# Calcolo della media mobile
if len(TD_errors) >= window_size:
    moving_avg_td = np.convolve(TD_errors, np.ones(window_size)/window_size, mode='valid')

    moving_avg_mean = np.mean(moving_avg_td)
    moving_avg_min = np.min(moving_avg_td)
    moving_avg_max = np.max(moving_avg_td)

    print(f"Media Mobile del TD Error (window={window_size}):")
    print(f" - Media: {moving_avg_mean:.4f}")
    print(f" - Minimo: {moving_avg_min:.4f}")
    print(f" - Massimo: {moving_avg_max:.4f}")

    # Analisi degli ultimi episodi della moving average
    if len(moving_avg_td) > 10:
        moving_avg_last_10_mean = np.mean(moving_avg_td[-10:])
        print(f"Media Mobile del TD Error negli ultimi 10 episodi: {moving_avg_last_10_mean:.4f}")

"""1. Nei primi episodi (intorno all’episodio 50) il valore medio del TD Error parte da circa 1.28 e mostra una rapida diminuzione, raggiungendo valori intorno a 0.50–0.60 entro l’episodio 100. Questa fase iniziale riflette un processo di apprendimento rapido in cui la rete sta ancora adattando le sue stime.

2. Con il progredire dell’addestramento, l’errore inizia a salire gradualmente: durante la seconda parte della fase di training il Moving Average del TD Error si stabilizza intorno a 1.0–1.2. Questo suggerisce che la rete ha raggiunto una certa stabilità, sebbene continui ad apportare aggiornamenti per affinare la propria politica.

3. Nella seconda metà dell’addestramento (episodi da 501 in poi) il TD Error medio mostra una tendenza a incrementarsi ulteriormente, arrivando a valori medi intorno a 1.5 e, verso la fine del training, plateau intorno a 1.8–2.0. Questo andamento indica che, sebbene il sistema abbia raggiunto la convergenza, permangono delle fluttuazioni dovute probabilmente alla variabilità intrinseca dell’ambiente e alla componente esplorativa.

**Sintesi dei Risultati della Media Mobile del TD Error**

- **Media Complessiva**

  La media della finestra mobile indica che, su periodi più lunghi, il modello mantiene una performance stabile con errori medi piuttosto contenuti.

- **Estremi**:

  Questi valori mostrano una certa variabilità, ma senza grandi fluttuazioni. Il massimo è comunque relativamente basso rispetto al TD Error complessivo.

- **Media Mobile Negli Ultimi 10 Episodi**

  Negli ultimi episodi, la media mobile rimane molto vicina alla media complessiva, suggerendo una performance coerente anche nel breve periodo.

**Conclusione**

La media mobile del TD Error conferma che il modello sta operando con errori contenuti e stabili nel tempo. Nonostante qualche fluttuazione, la performance risulta generalmente solida e non mostra segnali di problematiche o instabilità nei periodi recenti.
La diminuzione iniziale del TD Error conferma un rapido apprendimento all’inizio del training.

L’aumento successivo, fino a stabilizzarsi su valori medi relativamente bassi (intorno a 1.8–2.0), suggerisce che la rete ha affinato la propria politica e si è stabilizzata, pur mostrando lievi oscillazioni dovute alla natura stocastica dell’ambiente e alla continua esplorazione.
L’andamento complessivo evidenzia una convergenza del modello, con una fase iniziale di apprendimento rapido seguita da una fase di raffinamento in cui gli errori si mantengono a livelli moderati, indice di un modello stabile e ben addestrato.
"""

# Histogram of TD Errors
plt.figure(figsize=(8, 5))
plt.hist(TD_errors, bins=30, color='purple', edgecolor='black')
plt.xlabel("TD Error")
plt.ylabel("Frequenza")
plt.title("Distribuzione del TD Error")
plt.grid(True)
plt.show()

"""L'istogramma mostra una distribuzione non uniforme dei TD error ottenuti durante l'addestramento SARSA, con alcune evidenti concentrazioni a determinati intervalli:

- **Bassa fascia negativa**:

  Viene osservato un picco consistente (162 occorrenze) in questo intervallo, suggerendo che in numerosi episodi il TD error risultava moderatamente negativo. Questo potrebbe indicare situazioni in cui le stime di Q venivano leggermente sovrastimate.

- **Zona centrale attorno a zero**:
  
  Qui si riscontra un accumulo di 81 occorrenze, il che suggerisce che in molti episodi il TD error era vicino a zero, un segnale positivo di convergenza dove le previsioni del modello si allineavano bene con i ritorni osservati.

- **Fascia positiva intermedia**:

  Si nota un picco di 108 occorrenze in questo intervallo, indicativo di episodi in cui il modello sottostimava i ritorni e dove quindi il TD error era moderatamente positivo.

- **Alto TD Error**:

  Un primo picco significativo (230 occorrenze) si trova nell'intervallo 1.9051 - 2.0780, seguito da un picco ancora maggiore (361 occorrenze) nell'intervallo 2.9427 - 3.1156. Queste concentrazioni di valori elevati suggeriscono che in un numero consistente di episodi il modello ha commesso errori notevoli nel predire i ritorni, richiedendo aggiornamenti più sostanziali.

**Sintesi**:

L'istogramma evidenzia una distribuzione "bimodale" o fortemente concentrata in due regioni: una con TD error moderatamente negativo e un'altra con TD error molto elevato. La presenza di un picco attorno a zero è incoraggiante, in quanto indica che in molti episodi il modello ha raggiunto buone previsioni. Tuttavia, la forte concentrazione di TD error elevati in alcune fasce suggerisce che esistono episodi particolarmente difficili o situazioni in cui il modello deve effettuare correzioni significative. Questa variabilità potrebbe riflettere la complessità intrinseca dell'ambiente o la presenza di dinamiche di reward particolarmente sfidanti.

####Copertura dello Spazio degli Stati
"""

# Copertura dello Spazio degli Stati
plt.figure(figsize=(12, 5))
plt.plot(states_visited_per_episode, label='Stati Unici Visitati', color='purple')
plt.xlabel('Episodi')
plt.ylabel('Numero di Stati Visitati')
plt.title('Evoluzione della Copertura dello Spazio degli Stati')
plt.legend()
plt.grid(True)
plt.show()

"""L’andamento del numero di stati visitati suggerisce un processo di apprendimento che avviene per fasi, caratterizzato da periodi di stagnazione seguiti da improvvisi miglioramenti nell’esplorazione dell’ambiente.

- **Episodi 1-500**: Il numero di stati visitati rimane stabile a 24 per tutta questa fase, suggerendo che l'agente esplora solo una parte limitata dello spazio degli stati.
- **Episodi 501-502**: Ancora 24 stati visitati, senza cambiamenti immediati.
- **Episodi 503-546**: Il numero di stati visitati aumenta a 30, segnalando un'espansione improvvisa dell'esplorazione.
- **Episodi 547-778**: Un ulteriore incremento porta il totale a 33 stati, dopodiché si stabilizza.

Nel complesso, l’analisi mostra che l’apprendimento non è lineare, ma procede a step distinti, con fasi di stagnazione seguite da improvvisi miglioramenti. Questo comportamento è tipico negli algoritmi di reinforcement learning, dove l'agente può rimanere "bloccato" in strategie subottimali per un certo numero di episodi prima di riuscire a esplorare alternative migliori.
"""

# Coverage of State Space
unique_states_visited = [len(set(discretize_state(state) for state in episode_states)) for episode_states in states_per_episode]
plt.figure(figsize=(12, 5))
plt.plot(unique_states_visited, label="Stati Unici per Episodio", color='orange')
plt.xlabel("Episodi")
plt.ylabel("Numero di Stati Unici")
plt.title("Copertura dello Spazio degli Stati")
plt.legend()
plt.grid(True)
plt.show()

# Statistiche sul numero di stati unici visitati per episodio
mean_unique_states = np.mean(unique_states_visited)
median_unique_states = np.median(unique_states_visited)
std_unique_states = np.std(unique_states_visited)
max_unique_states = np.max(unique_states_visited)
min_unique_states = np.min(unique_states_visited)
total_unique_states = len(unique_states_visited)

print("=== Statistiche sulla Copertura dello Spazio degli Stati ===")
print(f"Media degli stati unici visitati per episodio: {mean_unique_states:.2f}")
print(f"Mediana degli stati unici visitati per episodio: {median_unique_states:.2f}")
print(f"Deviazione Standard: {std_unique_states:.2f}")
print(f"Massimo numero di stati unici visitati in un episodio: {max_unique_states}")
print(f"Minimo numero di stati unici visitati in un episodio: {min_unique_states}")
print(f"Numero totale di stati distinti visitati durante l'addestramento: {total_unique_states}")

"""Il grafico mostra l'evoluzione del numero di stati unici visitati dall'agente durante l'addestramento, con l'asse x che rappresenta gli episodi (da 1 a 1000) e l'asse y che indica il numero di stati unici esplorati in ciascun episodio.

- **Fase Iniziale** (Episodi 1–100): Nei primi episodi l’agente esplora in modo limitato, registrando valori bassi (da 3 a 12 stati unici). Questo indica che all'inizio l’agente sta ancora familiarizzando con l'ambiente e il proprio comportamento esplorativo è ancora in fase embrionale.

- **Incremento Moderato** (Episodi 105–166): A partire dall’episodio 105, si nota un leggero incremento, con la copertura che raggiunge circa 15 stati unici. Intorno all’episodio 166 il numero sale a 17 stati unici, suggerendo che l’agente sta ampliando il suo raggio di esplorazione.

- **Espansione Significativa** (Episodi 351–500): Un balzo più marcato si osserva intorno all’episodio 351, con la copertura che arriva a 20 stati unici e successivamente a 23–24 stati nei prossimi 100–150 episodi. Questa fase indica una transizione verso una scoperta più completa dell’ambiente.

- **Stabilizzazione Avanzata** (Episodi 501–1000): Nella seconda metà dell’addestramento, il numero di stati unici visitati aumenta ulteriormente, passando da circa 24 a 30, per poi stabilizzarsi intorno a 33–34 stati unici. Questo plateau suggerisce che l’agente ha quasi esplorato l’intero spazio degli stati disponibile, raggiungendo una copertura quasi completa.

In sintesi, il grafico evidenzia un percorso di apprendimento tipico: da una fase iniziale di esplorazione limitata, passando per un progressivo ampliamento della copertura, fino a raggiungere una fase di saturazione in cui la maggior parte degli stati dell'ambiente è stata visitata. Questo andamento conferma l’efficacia dell’approccio SARSA nel consentire all’agente di acquisire una comprensione completa dell’ambiente nel corso dell’addestramento.
"""

# Histogram of State Visit Frequency
state_visit_counts = {}
for episode_states in states_per_episode:
    for state in episode_states:
        state_key = discretize_state(state)
        state_visit_counts[state_key] = state_visit_counts.get(state_key, 0) + 1

plt.figure(figsize=(12, 5))
plt.hist(state_visit_counts.values(), bins=30, color='skyblue', edgecolor='black')
plt.xlabel("Frequenza di Visita")
plt.ylabel("Numero di Stati")
plt.title("Distribuzione della Frequenza degli Stati Visitati")
plt.grid(True)
plt.show()

# Analisi della distribuzione delle visite agli stati
state_visits = list(state_visit_counts.values())
mean_state_visits = np.mean(state_visits)
median_state_visits = np.median(state_visits)
std_state_visits = np.std(state_visits)
max_state_visits = np.max(state_visits)
min_state_visits = np.min(state_visits)

print("\n=== Statistiche sulla Frequenza di Visita degli Stati ===")
print(f"Media delle visite per stato: {mean_state_visits:.2f}")
print(f"Mediana delle visite per stato: {median_state_visits:.2f}")
print(f"Deviazione Standard: {std_state_visits:.2f}")
print(f"Massimo numero di visite per un singolo stato: {max_state_visits}")
print(f"Minimo numero di visite per un singolo stato: {min_state_visits}")

"""Il grafico evidenzia una distribuzione estremamente disomogenea: poche barre (stati) raggiungono frequenze molto elevate, mentre la maggior parte degli stati viene visitata solo sporadicamente (spesso 1 visita). Ciò indica che l’agente si concentra fortemente su poche aree, probabilmente quelle più “vantaggiose”, trascurando gran parte dello spazio degli stati.

####Analisi dei Q-values
"""

#Heatmap Q-values per gli Stati più Visitati
most_visited_states = sorted(state_visit_counts, key=state_visit_counts.get, reverse=True)[:20]
q_values = np.array([Q[state] for state in most_visited_states if state in Q])

plt.figure(figsize=(10, 6))
sns.heatmap(q_values, cmap='coolwarm', annot=False)
plt.xlabel("Azioni")
plt.ylabel("Stati più visitati")
plt.title("Heatmap dei Q-values per gli Stati più Visitati")
plt.show()

"""La heatmap dei Q-values evidenzia variazioni significative nei valori associati agli stati più visitati dall'agente. Notiamo che alcuni stati presentano Q-values relativamente alti, indicando scelte d'azione più vantaggiose in termini di reward previsto, mentre altri stati hanno valori negativi o prossimi allo zero, suggerendo azioni meno favorevoli o non ancora sufficientemente esplorate.

Un aspetto interessante è la presenza di alcuni picchi di Q-values (es. 2.90, 3.00), che indicano stati in cui l'agente ha appreso strategie efficaci per massimizzare la ricompensa. In contrasto, ci sono stati con valori negativi o molto bassi, il che potrebbe riflettere scenari penalizzanti o aree in cui l'agente ha ancora margini di miglioramento nell'esplorazione.

In generale, sembra esserci una tendenza a valori più elevati in stati con una certa configurazione ripetuta, suggerendo che l'agente ha trovato schemi ricorrenti favorevoli. Tuttavia, la dispersione dei valori indica che il processo di apprendimento potrebbe non essere ancora completamente stabile, con potenziali oscillazioni nei Q-values dovute alla strategia di aggiornamento scelta.

Un'ulteriore analisi potrebbe includere l'osservazione della variazione di questi valori nel tempo per capire se il modello sta ancora esplorando o se ha raggiunto una convergenza stabile.

####Confronto Best vs Worst Episode
"""

best_episode_idx = np.argmax(rewards_per_episode_sarsa)
worst_episode_idx = np.argmin(rewards_per_episode_sarsa)

plt.figure(figsize=(12, 5))
plt.plot(np.cumsum(rewards_per_step_per_episode[best_episode_idx]), label="Miglior Episodio", color='green')
plt.plot(np.cumsum(rewards_per_step_per_episode[worst_episode_idx]), label="Peggior Episodio", color='red')
plt.xlabel("Step")
plt.ylabel("Reward Accumulato")
plt.title("Confronto tra Episodio Migliore e Peggiore")
plt.legend()
plt.grid(True)
plt.show()

# Calcolo indice miglior e peggior episodio
best_episode_idx = np.argmax(rewards_per_episode_sarsa)
worst_episode_idx = np.argmin(rewards_per_episode_sarsa)

# Lista reward per step per il miglior e il peggior episodio
best_episode_rewards = rewards_per_step_per_episode[best_episode_idx]
worst_episode_rewards = rewards_per_step_per_episode[worst_episode_idx]

# Calcolo reward cumulativo per ogni step
best_cumulative = np.cumsum(best_episode_rewards)
worst_cumulative = np.cumsum(worst_episode_rewards)

print("=== Confronto tra Episodio Migliore e Peggiore ===\n")
print(f"Miglior Episodio (Indice {best_episode_idx}, Reward Totale = {rewards_per_episode_sarsa[best_episode_idx]:.2f}):")
for i, cum_reward in enumerate(best_cumulative, start=1):
    print(f" Step {i}: Reward Cumulativo = {cum_reward:.2f}")

print("\nPeggior Episodio (Indice {0}, Reward Totale = {1:.2f}):".format(
    worst_episode_idx, rewards_per_episode_sarsa[worst_episode_idx]))
for i, cum_reward in enumerate(worst_cumulative, start=1):
    print(f" Step {i}: Reward Cumulativo = {cum_reward:.2f}")

"""Il grafico mostra un confronto tra il miglior e il peggior episodio in termini di reward cumulativo ottenuto a ogni step.

- Nel **miglior episodio** (indice 1, reward totale = 3.00), osserviamo un andamento piatto nei primi due step, con un valore di reward cumulativo pari a zero. Tuttavia, al terzo step, si registra un improvviso incremento, portando il reward totale a 3.00. Questo comportamento suggerisce che l'agente abbia trovato rapidamente una strategia vincente, ottenendo il massimo guadagno in pochi passi.

- Nel **peggior episodio** (indice 17, reward totale = -2.00), il grafico presenta un andamento completamente diverso. Nei primi quattro step, il reward cumulativo rimane costante a zero, suggerendo una fase iniziale di esplorazione senza guadagni né penalità. A partire dal quinto step, però, il reward inizia a decrescere gradualmente, con penalità successive di -1 per step, fino a raggiungere il valore minimo di -2.00 al nono step. Questo trend negativo indica che l’agente ha intrapreso una sequenza di azioni subottimali, ricevendo penalità costanti senza riuscire a compensarle con decisioni migliori.

Il confronto tra le due curve evidenzia una forte discrepanza tra un episodio efficiente, caratterizzato da un guadagno netto in pochi step, e uno inefficace, in cui l’accumulo di penalità porta a un risultato complessivamente negativo.

##Sviluppo dell'Algoritmo DDQN

###Funzioni
"""

# Definizione rete DDQN
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.out = nn.Linear(128, action_dim)
        self.init_weights()

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.out(x)

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)

def select_action(state, epsilon):
    if random.random() < epsilon:
        return env.action_space.sample()
    else:
        state_tensor = torch.tensor(state.flatten(), dtype=torch.float32).unsqueeze(0).to(device)
        state_tensor = (state_tensor - state_tensor.mean()) / (state_tensor.std() + 1e-8)  # Normalizzazione
        with torch.no_grad():
            q_values = policy_net(state_tensor)
        return int(torch.argmax(q_values).item())

def optimize_model():
    if len(memory) < batch_size:
        return

    transitions = random.sample(memory, batch_size)

    state_batch = torch.tensor(np.array([s.flatten() for s, _, _, _, _ in transitions]), dtype=torch.float32).to(device)
    action_batch = torch.tensor([a for _, a, _, _, _ in transitions], dtype=torch.int64).unsqueeze(1).to(device)
    reward_batch = torch.tensor([r for _, _, r, _, _ in transitions], dtype=torch.float32).unsqueeze(1).to(device)
    next_state_batch = torch.tensor(np.array([ns.flatten() for _, _, _, ns, _ in transitions]), dtype=torch.float32).to(device)
    done_batch = torch.tensor([d for _, _, _, _, d in transitions], dtype=torch.float32).unsqueeze(1).to(device)

    # Normalizzazione degli stati
    state_batch = (state_batch - state_batch.mean()) / (state_batch.std() + 1e-8)
    next_state_batch = (next_state_batch - next_state_batch.mean()) / (next_state_batch.std() + 1e-8)

    q_values = policy_net(state_batch).gather(1, action_batch)
    next_actions = policy_net(next_state_batch).argmax(1, keepdim=True)
    next_q_values = target_net(next_state_batch).gather(1, next_actions)
    target_q_values = reward_batch + gamma * next_q_values * (1 - done_batch)

    loss = criterion(q_values, target_q_values)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    td_errors.append(loss.item())

"""###Addestramento"""

# Inizializzazione ambiente
state_dim = 4 * 10
action_dim = env.action_space.n

# Inizializzazione delle reti policy e target
policy_net = DQN(state_dim, action_dim).to(device)
target_net = DQN(state_dim, action_dim).to(device)
target_net.load_state_dict(policy_net.state_dict())
target_net.eval()

optimizer = optim.Adam(policy_net.parameters(), lr=0.001)
criterion = nn.SmoothL1Loss()  # Huber Loss per maggiore stabilità

memory = deque(maxlen=10000)
batch_size = 64

# Parametri addestramento
num_episodes = 1000
gamma = 0.99
epsilon = 1.0
min_epsilon = 0.01
epsilon_decay = 0.995
tau = 0.005  # Soft Target Update

# Metriche monitoraggio
rewards_per_episode_ddqn = []
epsilon_history_ddqn = []
td_errors = []
q_values_history = []
actions_per_episode_ddqn = []
rewards_per_step_per_episode_ddqn = []
state_visit_counts_ddqn_list = []

for episode in range(num_episodes):
    state, _ = env.reset()
    done = False
    total_reward = 0
    actions_count = np.zeros(action_dim, dtype=int)

    rewards_per_step = []
    state_visit_counts_ddqn = {}

    while not done:
        action = select_action(state, epsilon)
        actions_count[action] += 1

        if not isinstance(action, (tuple, list)):
            action = (action, action)

        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated

        reward_scalar = float(reward) if isinstance(reward, (int, float)) else float(reward[0])
        total_reward += reward_scalar
        rewards_per_step.append(reward_scalar)

        state_key = tuple(state.flatten())
        state_visit_counts_ddqn[state_key] = state_visit_counts_ddqn.get(state_key, 0) + 1

        memory.append((state, action[0], reward_scalar, next_state, done))
        state = next_state
        optimize_model()

    epsilon = max(min_epsilon, epsilon * epsilon_decay)

    # Soft Target Update
    for target_param, policy_param in zip(target_net.parameters(), policy_net.parameters()):
        target_param.data.copy_(tau * policy_param.data + (1 - tau) * target_param.data)

    rewards_per_episode_ddqn.append(total_reward)
    epsilon_history_ddqn.append(epsilon)
    actions_per_episode_ddqn.append(actions_count.copy())
    rewards_per_step_per_episode_ddqn.append(rewards_per_step)
    state_visit_counts_ddqn_list.append(state_visit_counts_ddqn)

    state_tensor = torch.tensor(state.flatten(), dtype=torch.float32).unsqueeze(0).to(device)
    state_tensor = (state_tensor - state_tensor.mean()) / (state_tensor.std() + 1e-8)  # Normalizzazione
    with torch.no_grad():
        q_values_history.append(policy_net(state_tensor).mean().item())

    if (episode + 1) % 100 == 0:
        print(f"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}")

print("Addestramento DDQN completato!")

"""###Analisi e Valutazioni

####Analisi del Reward
"""

# Cumulative Reward
cumulative_rewards_ddqn = np.cumsum(rewards_per_episode_ddqn)
plt.figure(figsize=(12, 5))
plt.plot(cumulative_rewards_ddqn, label="Cumulative Reward", color='purple')
plt.xlabel("Episodi")
plt.ylabel("Cumulative Reward")
plt.title("Cumulative Reward over Episodes (DDQN)")
plt.legend()
plt.grid(True)
plt.show()

"""Il grafico mostra l'andamento della ricompensa cumulativa per episodio durante l'addestramento del DDQN. Si osserva un aumento generale della ricompensa nel tempo, con una crescita quasi lineare nelle prime fasi, seguita da una leggera stabilizzazione e alcune fluttuazioni nelle fasi successive.

Nei primi episodi, la ricompensa cresce rapidamente, suggerendo che l'agente sta imparando strategie migliori per massimizzare il reward. Tuttavia, dopo un certo punto, si notano oscillazioni più frequenti, che potrebbero indicare esplorazione residua o variazioni nelle strategie apprese.

L'andamento complessivo suggerisce che l'agente ha raggiunto una politica relativamente stabile, con progressi consistenti nel lungo periodo, anche se alcune fluttuazioni potrebbero indicare la necessità di ulteriori aggiustamenti nei parametri di training, come il learning rate o il tasso di esplorazione.
"""

# Reward per Episodio con Moving Average
window_size = 50
plt.figure(figsize=(12, 5))
plt.plot(rewards_per_episode_ddqn, label="Reward per Episodio (Raw)", alpha=0.5)
if len(rewards_per_episode_ddqn) >= window_size:
    moving_avg_ddqn = np.convolve(rewards_per_episode_ddqn, np.ones(window_size)/window_size, mode='valid')
    plt.plot(range(window_size - 1, len(rewards_per_episode_ddqn)), moving_avg_ddqn,
             label=f"Moving Average (window={window_size})", color='red')
plt.xlabel("Episodi")
plt.ylabel("Reward Totale")
plt.title("Reward per Episodio con Moving Average (DDQN)")
plt.legend()
plt.grid(True)
plt.show()

"""Osservando la curva della Moving Average del Reward (finestra = 50) dall’episodio 50 al 1000, notiamo un andamento non lineare con diverse fasi. Inizialmente, la performance è piuttosto bassa evidenziando un inizio di apprendimento modesto. Con il progredire degli episodi fino a circa l’episodio 300, la curva mostra una tendenza al rialzo, raggiungendo valori attorno a 2, sebbene con oscillazioni e qualche calo temporaneo. Successivamente, tra gli episodi 300 e 350, c’è un breve periodo di ribasso, seguito da una ripresa che porta a valori leggermente più elevati intorno all’episodio 450. Tuttavia, nel tratto successivo la tendenza complessiva sembra invertire, con una progressiva diminuzione della Moving Average.

In sintesi, il grafico evidenzia un miglioramento iniziale della performance che, dopo aver raggiunto un picco intermedio, subisce un progressivo declino nelle fasi finali dell’addestramento. Questo andamento potrebbe suggerire fenomeni di overfitting o l’effetto di dinamiche ambientali che modificano la capacità dell’agente di mantenere performance elevate a lungo termine.
"""

# Rolling Variance dei Reward
if len(rewards_per_episode_ddqn) >= window_size:
    rolling_variance_ddqn = [np.var(rewards_per_episode_ddqn[i:i+window_size])
                             for i in range(len(rewards_per_episode_ddqn) - window_size + 1)]
    plt.figure(figsize=(12, 5))
    plt.plot(rolling_variance_ddqn, label=f"Rolling Variance (window={window_size})", color='orange')
    plt.xlabel("Episodi (indice finestra)")
    plt.ylabel("Variance of Reward")
    plt.title("Rolling Variance of Reward")
    plt.legend()
    plt.grid(True)
    plt.show()

"""Nella prima parte, il grafico mostra inizialmente una variabilità elevata dei reward, che poi tende a ridursi progressivamente fino a scendere nei periodi centrali. Questo andamento indica che, inizialmente, l'agente sperimenta una grande dispersione nelle performance, mentre con l'avanzare dell'addestramento il comportamento diventa più coerente e stabile.

Nella seconda parte, invece, la variabilità si attesta in media intorno a 2.0–2.4, con alcune fluttuazioni e picchi temporanei. Questo suggerisce che, pur avendo consolidato una certa stabilità, l'agente continua a mostrare una variabilità residua nei reward, probabilmente dovuta alla continua esplorazione o all'adattamento a dinamiche ambientali ancora in evoluzione.

In sintesi, il grafico evidenzia un trend non monotono: una forte variabilità iniziale che diminuisce nel mezzo dell'addestramento, seguita da un graduale aumento nei reward variance verso la fine. Questo comportamento riflette il passaggio da una fase di esplorazione intensiva a una fase in cui, pur essendo il comportamento più stabilizzato, permangono fluttuazioni dovute alla natura intrinsecamente stocastica dell'ambiente o a una strategia di esplorazione bilanciata.
"""

# Istogramma della Distribuzione dei Reward
plt.figure(figsize=(8, 5))
plt.hist(rewards_per_episode_ddqn, bins=30, color='skyblue', edgecolor='black')
plt.xlabel("Reward Totale")
plt.ylabel("Frequenza")
plt.title("Distribuzione del Reward Totale per Episodio (DDQN)")
plt.grid(True)
plt.show()

"""Il grafico evidenzia una distribuzione fortemente concentrata su due picchi. In particolare, la fascia più alta registra il massimo numero di occorrenze, seguita da un altro picco notevole nella fascia precedente. Questi due picchi indicano che la maggior parte degli episodi tende a raggiungere reward elevati, intorno a 2–3, suggerendo una performance generalmente positiva dell’agente.

Tuttavia, si osserva anche una presenza non trascurabile di episodi con reward negativi. Questi dati evidenziano che, pur con una performance media positiva, l’agente presenta una coda negativa significativa, che probabilmente deriva da episodi in cui il comportamento non è stato ottimale.

In sintesi, il grafico mostra una distribuzione bimodale con una forte concentrazione di reward elevati e una coda di episodi meno performanti, riflettendo un sistema in cui la maggior parte delle esecuzioni convergono verso buoni risultati, ma non mancano occasioni in cui l’agente ottiene risultati negativi.
"""

# Statistiche Riassuntive dei Reward
mean_reward_ddqn = np.mean(rewards_per_episode_ddqn)
median_reward_ddqn = np.median(rewards_per_episode_ddqn)
std_reward_ddqn = np.std(rewards_per_episode_ddqn)
max_reward_ddqn = np.max(rewards_per_episode_ddqn)
min_reward_ddqn = np.min(rewards_per_episode_ddqn)
print("Statistiche del Reward Totale per Episodio (DDQN):")
print(f"Media: {mean_reward_ddqn:.2f}")
print(f"Mediana: {median_reward_ddqn:.2f}")
print(f"Deviazione Standard: {std_reward_ddqn:.2f}")
print(f"Massimo: {max_reward_ddqn:.2f}")
print(f"Minimo: {min_reward_ddqn:.2f}")

"""Guardando queste statistiche, notiamo che il reward totale per episodio varia ampiamente, da un minimo di -2.00 fino a un massimo di 3.00. La mediana a 2.00 indica che metà degli episodi supera questo valore, mentre la media a 1.69, leggermente inferiore, suggerisce la presenza di episodi con performance particolarmente negative che abbassano il valore medio. La deviazione standard di 1.46 evidenzia una discreta variabilità nei risultati, confermando che, pur essendo la maggior parte degli episodi relativamente positivi, esistono delle eccezioni che influenzano la distribuzione complessiva del reward.

**Possibili interpretazioni**

La variabilità del reward e la presenza di valori negativi potrebbero essere dovuti a policy instabile, overfitting o esplorazione insufficiente.
La convergenza su poche azioni (osservata nella distribuzione delle azioni) potrebbe aver portato a strategie subottimali.
Un miglioramento potrebbe essere ottenuto con più esplorazione (modificando il decay dell’epsilon o introducendo rumore nella policy).

####Analisi delle Azioni
"""

# Calcolo distribuzione azioni
ddqn_action_sums = np.zeros(action_dim, dtype=int)
for episode_actions in actions_per_episode_ddqn:
    for action in episode_actions:
        action = int(action)
        ddqn_action_sums[action] += 1

plt.figure(figsize=(6,4))
plt.bar(range(action_dim), ddqn_action_sums, color='salmon', edgecolor='black')
plt.xlabel("Azione")
plt.ylabel("Frequenza")
plt.title("Distribuzione delle Azioni Eseguite")
plt.grid(True)
plt.show()

"""Il grafico mostra chiaramente una distribuzione fortemente sbilanciata delle azioni. L'azione 0 domina con oltre 16000 occorrenze, seguita da quella 1 (oltre 2000 occorrenze) e, in misura molto minore, dalle azioni 2 e 3. Le restanti azioni risultano praticamente inesistenti, con l'azione 4 che appare leggermente visibile e le altre non selezionate affatto. Questo andamento indica che l'agente ha sviluppato una strategia altamente concentrata su poche azioni, privilegiando quasi esclusivamente l'azione 0, probabilmente perché essa massimizza il reward nell'ambiente di addestramento.

####Analisi della Lunghezza degli Episodi
"""

# Calcolo numero step per episodio
steps_per_episode_ddqn = [len(rewards) for rewards in rewards_per_step_per_episode_ddqn]

plt.figure(figsize=(12,5))
plt.plot(steps_per_episode_ddqn, label="Steps per Episode", color='brown')
plt.xlabel("Episodi")
plt.ylabel("Numero di Steps")
plt.title("Numero di Steps per Episodio")
plt.legend()
plt.grid(True)
plt.show()

# Calcolo delle statistiche di base
tot_episodi = len(steps_per_episode_ddqn)
mean_steps = np.mean(steps_per_episode_ddqn)
min_steps = np.min(steps_per_episode_ddqn)
max_steps = np.max(steps_per_episode_ddqn)
correlation = np.corrcoef(steps_per_episode_ddqn, rewards_per_episode_ddqn)[0, 1]

print("\nStatistiche Generali:")
print(f"Numero totale di episodi: {tot_episodi}")
print(f"Media Steps per Episode: {mean_steps:.2f}")
print(f"Minimo Steps per Episode: {min_steps}")
print(f"Massimo Steps per Episode: {max_steps}")
print(f"Correlazione tra Steps e Reward: {correlation:.2f}")

"""- **Tendenza Centrale**: Osserviamo che il numero medio di step per episodio si attesta intorno a 4, con una mediana di 3.00. Questo suggerisce che la maggior parte degli episodi si risolve rapidamente, con una lieve asimmetria verso episodi più lunghi.

- **Variabilità**: La distribuzione non è strettamente concentrata attorno alla media, segnalando la presenza di episodi con una durata più elevata.

- **Punti Estremi**: L’episodio più corto si risolve in 1 step, indicando che l'agente è in grado di prendere decisioni immediate in alcuni casi. Tuttavia, il picco massimo di 11 step suggerisce la presenza di situazioni più complesse che richiedono maggiore esplorazione.

**Interpretazione**

La distribuzione mostra una prevalenza di episodi brevi, ma con alcuni outlier più lunghi. Questo potrebbe indicare che, mentre l'agente ha appreso strategie efficienti per molti scenari, in alcuni casi incontra ancora difficoltà nell'ottimizzazione della sua strategia decisionale. Approfondire questi casi potrebbe fornire spunti per ulteriori miglioramenti nel training.
"""

# Scatter plot: Reward per episodio vs. lunghezza degli episodi
plt.figure(figsize=(12,5))
plt.scatter(steps_per_episode_ddqn, rewards_per_episode_ddqn, alpha=0.7, color='green')
plt.xlabel("Steps per Episode")
plt.ylabel("Reward per Episode")
plt.title("Reward vs. Episode Length")
plt.grid(True)
plt.show()

# Analisi correlazione reward per episodio e lunghezza episodio
correlation = np.corrcoef(steps_per_episode_ddqn, rewards_per_episode_ddqn)[0,1]
print("\n=== Correlazione tra Steps per Episodio e Reward ===")
print(f"Correlazione Pearson: {correlation:.4f}")

# Episodi rappresentativi
best_episode_idx = np.argmax(rewards_per_episode_ddqn)
worst_episode_idx = np.argmin(rewards_per_episode_ddqn)

print("\n=== Esempi di Episodi ===")
print(f"Miglior Episodio (indice {best_episode_idx}): {steps_per_episode_ddqn[best_episode_idx]} steps, Reward = {rewards_per_episode_ddqn[best_episode_idx]:.2f}")
print(f"Peggior Episodio (indice {worst_episode_idx}): {steps_per_episode_ddqn[worst_episode_idx]} steps, Reward = {rewards_per_episode_ddqn[worst_episode_idx]:.2f}")

"""**Forza della Correlazione**

 Il coefficiente di correlazione di Pearson è 0.2462, indicando una correlazione positiva ma debole tra il numero di step per episodio e il reward. Questo significa che all'aumentare della durata di un episodio, il reward tende a crescere leggermente, ma la relazione non è particolarmente forte o lineare.

**Distribuzione dei Risultati**:

- Miglior Episodio (Indice 1): Con soli 3 step, l'agente ha ottenuto un reward di 3.00, dimostrando un comportamento efficiente e ottimizzato.
- Peggior Episodio (Indice 82): In 6 step, l'agente ha accumulato un reward negativo di -2.00, suggerendo che una maggiore durata dell'episodio non ha necessariamente portato a una strategia più efficace.

**Interpretazione**

Sebbene in alcuni casi episodi più lunghi possano portare a decisioni migliori, la correlazione relativamente bassa suggerisce che la durata dell'episodio da sola non è un indicatore chiaro della qualità della strategia adottata. L'agente potrebbe ancora incorrere in azioni subottimali anche in episodi più lunghi, rendendo utile un'analisi più approfondita delle dinamiche di apprendimento.

####Evoluzione di Epsilon
"""

plt.figure(figsize=(12, 5))
plt.plot(epsilon_history_ddqn, label="Valore di Epsilon", color='green')
plt.xlabel("Episodi")
plt.ylabel("Epsilon")
plt.title("Evoluzione di Epsilon durante l'Addestramento (DDQN)")
plt.legend()
plt.grid(True)
plt.show()

"""Il grafico mostra che il valore di epsilon parte leggermente sotto 1, indicando una forte propensione all’esplorazione, e si riduce progressivamente fino a raggiungere quasi 0, segnalando che l'agente passa ad una strategia prevalentemente greedy.
L'intervallo di Valori conferma il processo di riduzione graduale di epsilon nel corso degli episodi.
Questo andamento decrescente evidenzia come l’agente, inizialmente incline ad esplorare il più possibile, gradualmente si orienti verso l’exploitation delle conoscenze acquisite, seguendo la tipica strategia ε-greedy.

####Analisi del TD
"""

# Liste monitoraggio
TD_errors_ddqn = []
visited_states_ddqn = set()
Q_values_mean_ddqn = []
Q_values_std_ddqn = []
states_visited_per_episode_ddqn = []

# Finestra mobile TD Error
window_size_td_ddqn = 10

if len(TD_errors_ddqn) >= window_size_td_ddqn:
    smoothed_td_errors_ddqn = np.convolve(TD_errors_ddqn, np.ones(window_size_td_ddqn) / window_size_td_ddqn, mode='valid')
else:
    smoothed_td_errors_ddqn = TD_errors_ddqn

for episode in range(num_episodes):
    state, _ = env.reset()
    done = False
    episode_TD_errors = []

    while not done:
        action = select_action(state, epsilon)  # Funzione definita nell'addestramento
        action_tuple = (action, action) if not isinstance(action, (tuple, list)) else action

        next_state, reward, terminated, truncated, _ = env.step(action_tuple)
        done = terminated or truncated

        # Conversione reward
        reward_scalar = float(reward) if isinstance(reward, (int, float)) else float(reward[0])

        # Calcolo TD Error per DDQN
        state_tensor = torch.tensor(state.flatten(), dtype=torch.float32).unsqueeze(0).to(device)
        next_state_tensor = torch.tensor(next_state.flatten(), dtype=torch.float32).unsqueeze(0).to(device)

        with torch.no_grad():
            current_q_value = policy_net(state_tensor)[0, action]
            if done:
                target_value = reward_scalar
            else:
                next_action = policy_net(next_state_tensor).argmax(dim=1).item()
                target_value = reward_scalar + gamma * target_net(next_state_tensor)[0, next_action]

        td_error = abs(target_value - current_q_value.item())
        episode_TD_errors.append(td_error)

        visited_states_ddqn.add(tuple(state.flatten()))
        state = next_state

    TD_errors_ddqn.append(np.mean(episode_TD_errors))
    states_visited_per_episode_ddqn.append(len(visited_states_ddqn))
    Q_values_mean_ddqn.append(np.mean([policy_net(torch.tensor(s, dtype=torch.float32).unsqueeze(0).to(device)).mean().item() for s in visited_states_ddqn]))
    Q_values_std_ddqn.append(np.std([policy_net(torch.tensor(s, dtype=torch.float32).unsqueeze(0).to(device)).std().item() for s in visited_states_ddqn]))

# Statistiche generali sul TD Error
td_mean_ddqn = np.mean(TD_errors_ddqn)
td_min_ddqn = np.min(TD_errors_ddqn)
td_max_ddqn = np.max(TD_errors_ddqn)
td_std_ddqn = np.std(TD_errors_ddqn)

print(f"TD Error Medio: {td_mean_ddqn:.4f}")
print(f"TD Error Minimo: {td_min_ddqn:.4f}")
print(f"TD Error Massimo: {td_max_ddqn:.4f}")
print(f"TD Error Deviazione Standard: {td_std_ddqn:.4f}")

# Analisi dell'andamento nel tempo
if len(TD_errors_ddqn) > 10:
    td_last_10_mean_ddqn = np.mean(TD_errors_ddqn[-10:])
    print(f"TD Error Medio negli ultimi 10 episodi: {td_last_10_mean_ddqn:.4f}")

"""**Analisi della Variabilità**

La media complessiva di 0.6656 indica che, in media, il modello presenta un errore moderato.
Tuttavia, la presenza di valori estremi (minimo 0.1355 e massimo 3.8220) e una deviazione standard elevata (0.9351) evidenziano una notevole variabilità tra le transizioni.

**Andamento Recente**

La media del TD Error negli ultimi 10 episodi è salita a 1.2618, un valore significativamente più alto rispetto alla media globale.
Questo incremento suggerisce che nelle fasi recenti il modello potrebbe aver incontrato situazioni particolarmente complesse o episodi di instabilità.

**Implicazioni e Considerazioni**:

- **Stabilità**: La variabilità generale, con la presenza di episodi a TD Error elevato, potrebbe indicare momenti in cui il modello ha difficoltà a fare previsioni accurate, forse a causa di transizioni non ancora apprese o di una forte esplorazione.
- **Ottimizzazione**: L’aumento del TD Error medio negli ultimi 10 episodi richiede un’attenzione particolare, in quanto potrebbe essere utile rivedere alcuni parametri (come il learning rate, il meccanismo di esplorazione o altre configurazioni del modello) per migliorare la stabilità dell’apprendimento.
- **Approfondimento**: Analizzare in dettaglio gli episodi con TD Error elevato potrebbe fornire indicazioni preziose sulle situazioni problematiche, permettendo di intervenire miratamente per ottimizzare l’addestramento.

In conclusione, pur mantenendo un TD Error medio moderato durante la maggior parte degli episodi, i picchi e l’aumento recente indicano che esiste ancora margine per migliorare la stabilità e l’efficacia dell’apprendimento del modello.
Un monitoraggio continuo e un’analisi approfondita delle transizioni problematiche saranno fondamentali per intervenire e affinare il processo di training.
"""

plt.figure(figsize=(12, 5))
plt.plot(TD_errors_ddqn, label="TD Error Medio per Episodio", color='blue')
if len(smoothed_td_errors_ddqn) > 1:
    plt.plot(range(len(smoothed_td_errors_ddqn)), smoothed_td_errors_ddqn, label="TD Error Smoothed", color='red', linestyle='dashed')
plt.xlabel("Episodi")
plt.ylabel("TD Error")
plt.title("Andamento del TD Error durante l'Addestramento di DDQN")
plt.legend()
plt.grid(True)
plt.show()

"""Nel grafico dei TD Error, possiamo osservare una notevole variabilità nei valori attraverso gli episodi, con fluttuazioni sia alte che basse. Le righe più alte di TD Error (come nei casi con valori vicini a 3.3539 e 3.8220) indicano episodi in cui l'errore di temporizzazione è significativamente più elevato, suggerendo una possibile difficoltà del modello nel fare previsioni accurate in quei casi specifici. Questi picchi possono essere correlati a situazioni complesse o a cambiamenti improvvisi nell'ambiente di apprendimento, che potrebbero confondere il modello.

Al contrario, nei momenti in cui i TD Error sono più bassi (vicino a 0.1649 o 0.2592), possiamo osservare una maggiore stabilità del modello, suggerendo che l'apprendimento sta convergendo correttamente e il modello sta generalizzando bene a situazioni simili a quelle già incontrate.

Nonostante la presenza di picchi, che potrebbero segnalare episodi di maggiore difficoltà o apprendimento instabile, la frequente ripetizione di valori bassi suggerisce una buona base di convergenza del modello. Questo potrebbe essere un indicatore positivo dell'efficacia del processo di allenamento, con margini di miglioramento che si concentrano sulla gestione degli episodi più difficili.

In generale, una tendenza al ribasso nei TD Error sarebbe un segnale favorevole, ma sarebbe utile esplorare ulteriormente i picchi per capire se sono legati a errori nel training o a dinamiche particolari nell'ambiente.
"""

# Moving Average of TD Error
window_size = 50
if len(TD_errors) >= window_size:
    moving_avg_td = np.convolve(TD_errors_ddqn, np.ones(window_size)/window_size, mode='valid')
    plt.figure(figsize=(12, 5))
    plt.plot(moving_avg_td, label=f"Moving Avg TD Error (window={window_size})", color='blue')
    plt.xlabel("Episodi")
    plt.ylabel("TD Error")
    plt.title("Andamento del TD Error")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Calcolo della media mobile
if len(TD_errors_ddqn) >= window_size:
    moving_avg_td_ddqn = np.convolve(TD_errors_ddqn, np.ones(window_size)/window_size, mode='valid')

    moving_avg_mean = np.mean(moving_avg_td)
    moving_avg_min = np.min(moving_avg_td)
    moving_avg_max = np.max(moving_avg_td)

    print(f"Media Mobile del TD Error (window={window_size}):")
    print(f" - Media: {moving_avg_mean:.4f}")
    print(f" - Minimo: {moving_avg_min:.4f}")
    print(f" - Massimo: {moving_avg_max:.4f}")

    # Analisi degli ultimi episodi della moving average
    if len(moving_avg_td_ddqn) > 10:
        moving_avg_last_10_mean = np.mean(moving_avg_td_ddqn[-10:])
        print(f"Media Mobile del TD Error negli ultimi 10 episodi: {moving_avg_last_10_mean:.4f}")

"""Il grafico mostra un andamento interessante e articolato durante l'addestramento del DDQN:
1. **Fase Iniziale e Decrescita**
  
  Nei primi 100–150 episodi, l’errore medio si attesta intorno a valori di circa 0.65 e tende gradualmente a scendere, segnalando che l'agente sta imparando a fare previsioni più accurate.

2. **Stabilità e Bassa Variabilità**

  Nella parte centrale del training, il TD error medio si stabilizza intorno a valori compresi tra 0.55 e 0.60 in molti tratti, il che suggerisce che il modello ha raggiunto una certa stabilità e che le sue previsioni sono abbastanza coerenti nei vari episodi.

3. **Fluttuazioni e Picchi Successivi**

  Successivamente, si osservano alcune oscillazioni e incrementi, con periodi in cui il TD error aumenta fino a circa 0.80–0.90 e, verso la fine (intorno agli episodi 990–1000), arriva a circa 0.93. Questi picchi potrebbero indicare che l'agente sta incontrando situazioni nuove o particolarmente complesse, oppure che vi sono dei momenti di instabilità dovuti a variazioni nel comportamento esplorativo o a transizioni inaspettate nell'ambiente.

**Sintesi dei Risultati della Media Mobile del TD Error**

**Media Complessiva**

La media mobile complessiva è significativamente più alta rispetto ai risultati precedenti, indicando che il modello DDQN sta producendo errori medi più grandi.

**Estremi**

Questi valori indicano una variazione più ampia degli errori, con il massimo che arriva oltre l'1, suggerendo qualche fluttuazione notevole nella performance.

**Media Mobile Negli Ultimi 10 Episodi**

Negli ultimi episodi, la media mobile è più alta rispetto alla media complessiva, il che potrebbe indicare che il modello sta incontrando maggiori difficoltà o che ci sono transizioni tra stati che causano errori più grandi.

**Conclusione**

Il modello DDQN presenta una variabilità maggiore rispetto ad altri modelli, con errori medi superiori e un ampio range tra minimo e massimo. La performance nel breve periodo è più alta rispetto alla media complessiva, suggerendo che potrebbe esserci qualche difficoltà o instabilità nelle decisioni recenti del modello.

Nel complesso, sebbene la fase iniziale mostri un miglioramento significativo (errore in diminuzione), le fluttuazioni successive evidenziano la presenza di sfide o cambiamenti che il modello deve ancora apprendere completamente. Questo andamento suggerisce che, mentre il DDQN ha raggiunto una buona stabilità per gran parte del training, esistono comunque dei momenti in cui l’errore aumenta, offrendo spunti per possibili ulteriori ottimizzazioni (ad es. tuning dei parametri, aggiustamenti nell’esplorazione, ecc.).
"""

# Distribuzione del TD Error
plt.figure(figsize=(12, 5))
plt.hist(TD_errors_ddqn, bins=30, color='skyblue', edgecolor='black')
plt.xlabel("TD Error")
plt.ylabel("Frequenza")
plt.title("Distribuzione del TD Error")
plt.grid(True)
plt.show()

"""L'istogramma della distribuzione del TD Error mostra che la stragrande maggioranza delle transizioni durante l’addestramento presenta valori di errore piuttosto bassi. In particolare:

- **Intervallo principale**: Questi due intervalli, con 470 e 301 occorrenze rispettivamente, contano oltre 770 transizioni. Ciò indica che in gran parte degli episodi il modello produce un TD error contenuto, segno di un apprendimento stabile e di una buona convergenza delle stime di valore.

- **Occorrenze sporadiche a valori intermedi**: Ci sono pochissime transizioni nei range 0.40 – 0.50 e 0.50 – 0.60, segnalando che quasi nessun episodio genera errori in questi livelli intermedi.

- **Picchi in alcuni intervalli superiori**: Alcuni intervalli, evidenziano che in alcuni momenti il TD error raggiunge valori elevati. Questi picchi potrebbero indicare transizioni particolarmente difficili o situazioni in cui il modello ha incontrato stati complessi o imprevisti.

Nel complesso, la distribuzione suggerisce che mentre la maggior parte dei casi mostra una buona stima con errori bassi, esistono anche casi isolati con errori elevati che potrebbero essere analizzati per capire se e come migliorare ulteriormente il comportamento dell’agente.

####Copertura dello Spazio degli Stati
"""

# Copertura dello Spazio degli Stati
plt.figure(figsize=(12, 5))
plt.plot(states_visited_per_episode_ddqn, label='Stati Unici Visitati', color='purple')
plt.xlabel('Episodi')
plt.ylabel('Numero di Stati Visitati')
plt.title('Evoluzione della Copertura dello Spazio degli Stati')
plt.xticks(np.arange(0, len(states_visited_per_episode_ddqn), 100))
plt.legend()
plt.grid(True)
plt.show()

"""L’andamento della copertura dello spazio degli stati per l’agente DDQN evidenzia un processo di apprendimento a step distinti:

- **Episodi Iniziali**:
Nei primi episodi, l’agente mostra una copertura molto limitata, segnalando una fase in cui sta appena iniziando a interagire con l’ambiente e possiede una conoscenza ridotta dello spazio.

- **Primo Salto e Stabilizzazione**:
A partire dall’episodio 6, l’agente raggiunge rapidamente un plateau intorno ai 13 stati unici per episodio. Questo indica il primo stadio in cui l’agente ha appreso un set di stati comuni e stabilito una base iniziale di conoscenza.

- **Incrementi Progressivi**:
  
  Successivamente, l’esplorazione si amplia:

 - Intorno agli episodi 200-380 il numero di stati unici cresce fino a circa 22,
 - Segue un ulteriore salto circa a 24 stati,
 - E, da circa l’episodio 420, si osserva un aumento 25 stati uoltrenici.
  Questi incrementi riflettono una progressiva acquisizione di familiarità con l’ambiente, con l’agente capace di riconoscere sempre più stati.

- **Saturazione (Episodi >210)**:
In una fase avanzata, l’andamento continua con ulteriori incrementi per poi stabilizzarsi definitivamente intorno ai 50 stati per episodio. Questo plateau finale suggerisce che l’agente ha raggiunto una copertura quasi completa dello spazio degli stati accessibile durante l’addestramento.

Nel complesso il grafico mostra chiaramente un percorso evolutivo non lineare: dall’esplorazione iniziale limitata, l’agente passa attraverso fasi di rapidi miglioramenti e ampliamenti, fino a raggiungere una saturazione dell’ambiente. Questo comportamento conferma l’efficacia del modello nel mappare lo spazio degli stati, sebbene la saturazione finale a 54 stati possa indicare un limite intrinseco o la necessità di ulteriori strategie per espandere l’esplorazione.
"""

# Calcolo numero stati unici visitati per episodio
unique_states_ddqn = [len(d) for d in state_visit_counts_ddqn_list]

state_visit_counts_total = {}
for d in state_visit_counts_ddqn_list:
    for state_key, count in d.items():
        state_visit_counts_total[state_key] = state_visit_counts_total.get(state_key, 0) + count

plt.figure(figsize=(12,5))
plt.plot(unique_states_ddqn, label="Stati Unici per Episodio", color='orange')
plt.xlabel("Episodi")
plt.ylabel("Numero di Stati Unici")
plt.title("Copertura dello Spazio degli Stati")
plt.legend()
plt.grid(True)
plt.show()

# Statistiche sul numero di stati unici visitati per episodio
mean_unique_states_ddqn = np.mean(unique_states_ddqn)
median_unique_states_ddqn = np.median(unique_states_ddqn)
std_unique_states_ddqn = np.std(unique_states_ddqn)
max_unique_states_ddqn = np.max(unique_states_ddqn)
min_unique_states_ddqn = np.min(unique_states_ddqn)
total_unique_states_ddqn = len(state_visit_counts_total)

print("=== Statistiche sulla Copertura dello Spazio degli Stati ===")
print(f"Media degli stati unici visitati per episodio: {mean_unique_states_ddqn:.2f}")
print(f"Mediana degli stati unici visitati per episodio: {median_unique_states_ddqn:.2f}")
print(f"Deviazione Standard: {std_unique_states_ddqn:.2f}")
print(f"Massimo numero di stati unici visitati in un episodio: {max_unique_states_ddqn}")
print(f"Minimo numero di stati unici visitati in un episodio: {min_unique_states_ddqn}")
print(f"Numero totale di stati distinti visitati durante l'addestramento: {total_unique_states_ddqn}")

"""Il grafico mostra l'evoluzione del numero di stati unici visitati dall'agente durante l'addestramento:

- **Fase Iniziale**
  
  Nei primi episodi l’agente esplora un numero limitato di stati, evidenziando una conoscenza iniziale molto restrittiva dell’ambiente.

- **Espansione Graduale**
  
  Con il progredire dell’addestramento, l’agente aumenta la copertura:

 - Verso gli episodi 80–130, il numero sale a circa 16 stati unici per episodio.
 - Successivamente, si osserva un ulteriore incremento, con molti episodi che raggiungono 19 stati unici.
 - Questo andamento indica che l’agente sta ampliando la sua mappa dell’ambiente.

- T**ransizione e Saturazione Intermedia**

  La copertura si espande ulteriormente, raggiungendo valori intorno a 25 stati unici per episodio. In questa fase l’agente ha consolidato la conoscenza di una porzione significativa dello spazio, mostrando una maggiore stabilità nell’esplorazione.

- **Fase Avanzata e Saturazione Finale**
  
  A partire da circa l’episodio 500, si osserva un salto netto:

 - I valori iniziano a salire notevolmente e successivamente si raggiunge un plateau, con episodi che esplorano fino a 54 stati unici.
 - Verso la fine dell’addestramento, la copertura diventa costante, suggerendo che l’agente ha quasi completamente esplorato l’intero spazio degli stati disponibile.


In conclusione, il grafico evidenzia un chiaro percorso evolutivo: l’agente passa da una fase di esplorazione limitata a una copertura ampia e stabile dello spazio degli stati. Questa progressione conferma l’efficacia del DDQN nell’apprendere e mappare l’ambiente, con una saturazione finale che probabilmente rappresenta il limite intrinseco dello spazio esplorabile nel contesto attuale.
"""

# Istogramma della frequenza degli stati visitati in totale
plt.figure(figsize=(12,5))
plt.hist(list(state_visit_counts_total.values()), bins=30, color='skyblue', edgecolor='black')
plt.xlabel("Frequenza di Visita")
plt.ylabel("Numero di Stati")
plt.title("Distribuzione della Frequenza degli Stati Visitati (DDQN)")
plt.grid(True)
plt.show()

# Analisi della distribuzione delle visite agli stati
state_visits_ddqn = list(state_visit_counts_total.values())
mean_state_visits_ddqn = np.mean(state_visits_ddqn)
median_state_visits_ddqn = np.median(state_visits_ddqn)
std_state_visits_ddqn = np.std(state_visits_ddqn)
max_state_visits_ddqn = np.max(state_visits_ddqn)
min_state_visits_ddqn = np.min(state_visits_ddqn)

print("\n=== Statistiche sulla Frequenza di Visita degli Stati ===")
print(f"Media delle visite per stato: {mean_state_visits_ddqn:.2f}")
print(f"Mediana delle visite per stato: {median_state_visits_ddqn:.2f}")
print(f"Deviazione Standard: {std_state_visits_ddqn:.2f}")
print(f"Massimo numero di visite per un singolo stato: {max_state_visits_ddqn}")
print(f"Minimo numero di visite per un singolo stato: {min_state_visits_ddqn}")

"""Il grafico evidenzia una distribuzione molto asimmetrica nella frequenza di visita degli stati. In media, ogni stato viene visitato circa 10 volte, ma la mediana di soli 2 indica che la maggior parte degli stati viene esplorata poco frequentemente. La notevole deviazione standard (41.19) e il contrasto tra il massimo (495 visite) e il minimo (1 visita) sottolineano che l’agente tende a concentrarsi ripetutamente su alcuni stati strategicamente importanti, mentre altri vengono trascurati.

####Analisi Q-Values
"""

# Heatmap Q-values per gli Stati più Visitati
if 'state_visit_counts_ddqn' in globals():
    most_visited_states_ddqn = sorted(state_visit_counts_ddqn, key=state_visit_counts_ddqn.get, reverse=True)[:20]
    q_values_ddqn = np.array([Q[state] for state in most_visited_states_ddqn if state in Q])
    plt.figure(figsize=(10, 6))
    sns.heatmap(q_values_ddqn, cmap='coolwarm', annot=False)
    plt.xlabel("Azioni")
    plt.ylabel("Stati più visitati")
    plt.title("Heatmap dei Q-values per gli Stati più Visitati")
    plt.show()

"""Guardando la heatmap dei Q‐values per gli stati più visitati, emergono diverse osservazioni interessanti:

- **Variabilità nei Q‐value**s:

  Alcuni stati mostrano Q‐values relativamente contenuti (ad esempio, il primo stato ha valori che variano tra circa –0.16 e 0.16), mentre altri presentano valori più elevati, fino a circa 1.8 o anche superiori. Ciò indica che il modello attribuisce differenti aspettative di guadagno a seconda della configurazione dello stato.

- **Distribuzione dei valori**:

  Notiamo che per alcune configurazioni di stato – tipicamente caratterizzate da blocchi di “10” e “0” – il modello calcola Q‐values sia positivi che negativi. Questo contrasto suggerisce che il DDQN è in grado di discriminare tra azioni favorevoli e sfavorevoli in contesti simili, affinando le sue valutazioni.

- **Struttura degli stati**:

  I vettori di stato, con numeri preponderanti come “0” e “10” (e occasionalmente “1”), riflettono probabilmente caratteristiche rilevanti del problema (ad esempio, posizione o presenza di ostacoli/obiettivi). Le differenze nei Q‐values tra stati con configurazioni simili evidenziano come il modello possa essere sensibile a variazioni sottili nel contesto.

- **Implicazioni per l’agente**:
In definitiva, la heatmap mostra come il DDQN abbia sviluppato una “mappa” interna che differenzia chiaramente gli stati in base al potenziale di guadagno futuro. Gli stati che ottengono Q‐values elevati sono probabilmente associati a scelte che portano a ricompense maggiori, mentre quelli con valori negativi indicano situazioni rischiose o penalizzanti.

Questi risultati, presentati graficamente, offrono una visione immediata e dettagliata di come il modello valuti le varie situazioni dell’ambiente, confermando la capacità del DDQN di apprendere e discriminare efficacemente tra diversi scenari.

####Confronto Best vs Worst Episode
"""

if 'rewards_per_step_per_episode_ddqn' in globals():
    best_episode_idx_ddqn = np.argmax(rewards_per_episode_ddqn)
    worst_episode_idx_ddqn = np.argmin(rewards_per_episode_ddqn)

    plt.figure(figsize=(12, 5))
    plt.plot(np.cumsum(rewards_per_step_per_episode_ddqn[best_episode_idx_ddqn]), label="Miglior Episodio", color='green')
    plt.plot(np.cumsum(rewards_per_step_per_episode_ddqn[worst_episode_idx_ddqn]), label="Peggior Episodio", color='red')
    plt.xlabel("Step")
    plt.ylabel("Reward Accumulato")
    plt.title("Confronto Best vs Worst Episode (DDQN)")
    plt.legend()
    plt.grid(True)
    plt.show()
else:
    print("Dati per il confronto Best vs Worst Episode non disponibili per DDQN.")

if 'rewards_per_step_per_episode_ddqn' in globals():
    best_episode_idx_ddqn = np.argmax(rewards_per_episode_ddqn)
    worst_episode_idx_ddqn = np.argmin(rewards_per_episode_ddqn)

    best_episode_rewards = np.cumsum(rewards_per_step_per_episode_ddqn[best_episode_idx_ddqn])
    worst_episode_rewards = np.cumsum(rewards_per_step_per_episode_ddqn[worst_episode_idx_ddqn])

    print("=== Confronto Best vs Worst Episode (DDQN) ===")
    print(f"Indice del Miglior Episodio: {best_episode_idx_ddqn}")
    print(f"Indice del Peggior Episodio: {worst_episode_idx_ddqn}")

    print("\nMiglior Episodio - Reward accumulato per step:")
    for step, reward in enumerate(best_episode_rewards):
        print(f"Step {step + 1}: {reward:.2f}")

    print("\nPeggior Episodio - Reward accumulato per step:")
    for step, reward in enumerate(worst_episode_rewards):
        print(f"Step {step + 1}: {reward:.2f}")
else:
    print("Dati per il confronto Best vs Worst Episode non disponibili per DDQN.")

"""Il grafico evidenzia una netta differenza tra il miglior e il peggior episodio, mostrando le capacità dell'agente in condizioni favorevoli e sfavorevoli.

- **Miglior Episodio (Indice 0)**:
 - Il reward cresce rapidamente, raggiungendo +3.00 già al terzo step.
 - Indica una strategia efficace con decisioni ottimali sin dall'inizio.

- **Peggior Episodio (Indice 243)**:
 - Nessun guadagno nei primi quattro step, seguito da penalità (-1.00, -2.00).
 - L'agente ha compiuto scelte subottimali, portandosi in stati sfavorevoli.

- **Analisi Comparativa e Miglioramenti**:
 - Il divario suggerisce che l'agente non generalizza bene su tutti gli episodi.
 - Serve un miglior bilanciamento tra exploration ed exploitation.
 - Approfondire le dinamiche del peggior episodio per identificare pattern di errore.

Il grafico conferma che l'agente può eseguire strategie efficaci, ma necessita di maggiore stabilità per ridurre episodi negativi.

## Analisi e Visualizzazione di Confronto dei Risultati
"""

# -------------------------------
# Confronto Medie Mobili
# -------------------------------
window_size = 50
if len(rewards_per_episode_sarsa) >= window_size and len(rewards_per_episode_ddqn) >= window_size:
    sarsa_moving_avg = np.convolve(rewards_per_episode_sarsa, np.ones(window_size)/window_size, mode='valid')
    ddqn_moving_avg = np.convolve(rewards_per_episode_ddqn, np.ones(window_size)/window_size, mode='valid')

    plt.figure(figsize=(10,6))
    plt.plot(sarsa_moving_avg, label='SARSA (media mobile)', alpha=0.8, color='blue')
    plt.plot(ddqn_moving_avg, label='DDQN (media mobile)', alpha=0.8, color='orange')
    plt.xlabel("Episodi")
    plt.ylabel("Reward Totale (media mobile)")
    plt.title("Confronto delle Medie Mobili dei Reward: SARSA vs DDQN")
    plt.legend()
    plt.grid(True)
    plt.show()
else:
    print("Numero di episodi inferiore alla dimensione della finestra di smoothing.")

"""Il grafico mostra le curve della media mobile dei reward per episodio, confrontando SARSA e DDQN. Inizialmente, già intorno all'episodio 50, DDQN presenta valori leggermente superiori rispetto a SARSA. Con il progredire dell’addestramento, entrambe le curve crescono progressivamente; da circa episodio 500 in poi, il gap si riduce e i due algoritmi convergono verso valori medi compresi tra 1.9 e 2.0.

In sintesi, sebbene SARSA a tratti mostri performance marginalmente migliori, entrambi gli approcci tendono a stabilizzarsi su risultati simili nel lungo periodo.
"""

# -------------------------------
# Confronto Reward Medi
# -------------------------------
def calculate_avg_reward(rewards, num_episodes=100):
    if len(rewards) < num_episodes:
        return np.mean(rewards)  # Usa tutti i dati disponibili
    return np.mean(rewards[:num_episodes]), np.mean(rewards[-num_episodes:])

sarsa_initial_avg, sarsa_final_avg = calculate_avg_reward(rewards_per_episode_sarsa)
ddqn_initial_avg, ddqn_final_avg = calculate_avg_reward(rewards_per_episode_ddqn)

print("=== Confronto dei Reward Medi ===")
print(f"SARSA: Reward medio primi 100 episodi: {sarsa_initial_avg:.2f}")
print(f"SARSA: Reward medio ultimi 100 episodi: {sarsa_final_avg:.2f}")
print(f"DDQN:  Reward medio primi 100 episodi: {ddqn_initial_avg:.2f}")
print(f"DDQN:  Reward medio ultimi 100 episodi: {ddqn_final_avg:.2f}\n")

"""L'analisi dei reward medi mostra che:

- **SARSA** ha avuto un miglioramento, passando da 1.26 nei primi 100 episodi a 1.55 negli ultimi 100 episodi. Questo indica che l'agente ha appreso e migliorato la sua strategia nel tempo.
- **DDQN** è partito con un reward medio più alto rispetto a SARSA, ma ha mostrato un incremento minore, arrivando solo a 1.63 negli ultimi 100 episodi.

**Interpretazione**:
- **SARSA** sembra avere una curva di apprendimento più ripida, migliorando in modo consistente. Potrebbe essere più efficace in questo contesto.
- **DDQN** parte meglio, ma il miglioramento è più limitato. Potrebbe essere necessario un fine-tuning dell'algoritmo (ad esempio, modifiche nel target update o nell'epsilon decay).
- Se il reward massimo possibile è più alto di 1.63, c'è ancora margine di miglioramento per entrambi.

"""

#----------------------------------------------
# Confronto della Distribuzione delle Azioni
#-----------------------------------------------
sarsa_action_sums = np.zeros(env.action_space.n, dtype=int)
ddqn_action_sums = np.zeros(env.action_space.n, dtype=int)

for episode_actions in actions_per_episode_sarsa:
    for action in episode_actions:
        if 0 <= action < env.action_space.n:
            sarsa_action_sums[action] += 1

for episode_actions in actions_per_episode_ddqn:
    for action in episode_actions:
        action = int(action)  # Garantisce conversione sicura a intero
        if 0 <= action < env.action_space.n:
            ddqn_action_sums[action] += 1

print("=== Distribuzione delle Azioni durante l'Addestramento ===")
print(f"SARSA: {sarsa_action_sums}\n")
print(f"DDQN: {ddqn_action_sums}\n")

fig, ax = plt.subplots(1, 2, figsize=(12, 5))
ax[0].bar(range(len(sarsa_action_sums)), sarsa_action_sums, color='skyblue', edgecolor='black')
ax[0].set_xlabel("Azione")
ax[0].set_ylabel("Frequenza")
ax[0].set_title("Distribuzione Azioni SARSA")

ax[1].bar(range(len(ddqn_action_sums)), ddqn_action_sums, color='salmon', edgecolor='black')
ax[1].set_xlabel("Azione")
ax[1].set_ylabel("Frequenza")
ax[1].set_title("Distribuzione Azioni DDQN")

plt.tight_layout()
plt.show()

"""L'analisi della distribuzione delle azioni rivela una differenza significativa tra ***SARSA*** e ***DDQN***:

- **SARSA**
 - Le azioni sono distribuite in modo relativamente bilanciato su tutte le 20   possibili azioni.
 - Questo suggerisce che SARSA esplora attivamente diverse strategie e mantiene una varietà di decisioni.
 - L'ampia esplorazione potrebbe spiegare il suo miglioramento nel tempo, poiché trova gradualmente azioni più vantaggiose.

- **DDQN**
 - La maggior parte delle azioni è concentrata sulle prime due classi , con un rapido crollo nelle altre categorie.
 - Alcune azioni non vengono mai scelte (probabilmente perché l'algoritmo le ha giudicate non redditizie troppo presto).
 - Questo comportamento suggerisce che DDQN ha una forte politica deterministica e potrebbe aver trovato una strategia subottimale, riducendo troppo l'esplorazione.

**Conclusioni**
- **SARSA** mantiene una maggiore esplorazione e diversità nelle scelte, il che potrebbe spiegare il suo miglioramento più evidente.
- **DDQN** sembra convergere rapidamente su poche azioni, il che potrebbe essere segno di prematura convergenza o eccessiva sfruttamento (exploitation).

Potrebbe essere utile aumentare l'epsilon decay rate o introdurre strategie di esplorazione aggiuntive (come il noise nella policy) per migliorare DDQN.

##Commento Finale

Il progetto mira a rafforzare la sicurezza informatica nel settore sanitario, utilizzando algoritmi di Reinforcement Learning per simulare scenari di attacco e difesa in un ambiente specializzato (gym‑idsgame). In questo contesto, il confronto tra SARSA e DDQN offre spunti interessanti su come migliorare le strategie difensive:

- **Performance di Reward e Apprendimento**:

  I risultati indicano che SARSA, pur partendo da performance iniziali inferiori, mostra un miglioramento significativo – passando da un reward medio di circa 0.82 nei primi 100 episodi a 1.81 negli ultimi 100. Questo suggerisce che, nel tempo, SARSA riesce a sviluppare strategie più efficaci, probabilmente grazie a una maggiore esplorazione e a una politica di apprendimento più flessibile.
  Al contrario, il DDQN parte con un vantaggio iniziale (reward medio di 1.30) ma registra un incremento limitato, raggiungendo solo 1.40 negli ultimi 100 episodi. La correlazione debole (0.2156) tra il numero di passi e il reward evidenzia che, sebbene episodi più lunghi possano portare a ricompense migliori, la lunghezza dell’episodio da sola non è un indicatore affidabile della qualità della strategia.

- **Distribuzione delle Azioni e Copertura dello Spazio degli Stati**:

  L’analisi della distribuzione delle azioni rivela che SARSA adotta una politica più diversificata, esplorando attivamente tutte le possibili azioni. Questa ampia esplorazione consente di identificare vulnerabilità e di apprendere strategie difensive più robuste, un aspetto cruciale per proteggere i dati sensibili in ambito sanitario.
  DDQN, invece, tende a concentrarsi su poche azioni (in particolare le prime due classi), il che potrebbe portare a una convergenza prematura su strategie subottimali. Questa limitata diversità nelle scelte potrebbe compromettere l’adattabilità dell’agente di fronte ad attacchi informatici sempre più sofisticati.

- **Implicazioni per la Cyber Security**:

  Considerando la complessità crescente degli attacchi informatici nel settore sanitario, una maggiore esplorazione e flessibilità strategica – come quella mostrata da SARSA – risulta particolarmente vantaggiosa. Una politica di esplorazione bilanciata aiuta non solo a migliorare l’efficacia delle contromisure, ma anche a identificare e correggere potenziali vulnerabilità prima che possano essere sfruttate in scenari reali.
  Tuttavia, il DDQN offre un punto di partenza migliore in termini di reward iniziali, suggerendo che con un fine-tuning (ad esempio, rallentando l’epsilon decay o introducendo metodi di esplorazione aggiuntivi) potrebbe raggiungere performance simili o superiori a SARSA.

**Conclusioni**

Nel complesso, entrambi i modelli presentano punti di forza e aree di miglioramento. SARSA dimostra una curva di apprendimento più ripida e una maggiore capacità esplorativa, elementi chiave per sviluppare strategie difensive robuste in ambienti dinamici e complessi come quelli della sanità. Dall’altro lato, il DDQN, pur partendo con performance iniziali più elevate, potrebbe beneficiare di ulteriori ottimizzazioni per evitare una convergenza troppo rapida su poche azioni.
Questi risultati supportano l’obiettivo del progetto di migliorare la difesa informatica nel settore sanitario: identificare vulnerabilità e sviluppare strategie difensive efficaci. L’adozione di un approccio che combini i punti di forza di SARSA (ampia esplorazione) con la capacità iniziale del DDQN di ottenere reward migliori potrebbe rappresentare una soluzione innovativa e ottimale per affrontare le minacce informatiche sempre più sofisticate.
"""